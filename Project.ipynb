{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.83:7077\") \\\n",
    "        .appName(\"Mikaela_Project\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\", 1)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.5 ms, sys: 1.27 ms, total: 9.76 ms\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_02 = spark_session.read.json(\"hdfs://192.168.2.83:9000/user/ubuntu/input/RC_2009-10.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 ms, sys: 9.12 ms, total: 30.5 ms\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_03 = spark_session.read.json(\"hdfs://192.168.2.83:9000/user/ubuntu/input/RC_2011-06.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 ms, sys: 9.18 ms, total: 32.5 ms\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_04 = spark_session.read.json(\"hdfs://192.168.2.83:9000/user/ubuntu/input/RC_2011-07.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 90.6 ms, sys: 21.5 ms, total: 112 ms\n",
      "Wall time: 8min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_2017_01 = spark_session.read.json(\"hdfs://192.168.2.83:9000/user/ubuntu/input/RC_2017-01.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_init = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.2 ms, sys: 11.7 ms, total: 22.9 ms\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark_session.read.json(\"hdfs://192.168.2.83:9000/user/ubuntu/input/RC_2011-06.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+-----+------+------+-------+--------+----------+----------+------------+-----+------------+--------------------+------------+---+\n",
      "|archived|          author|author_flair_css_class|   author_flair_text|                body|controversiality|created_utc|distinguished|downs|edited|gilded|     id| link_id|      name| parent_id|retrieved_on|score|score_hidden|           subreddit|subreddit_id|ups|\n",
      "+--------+----------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+-----+------+------+-------+--------+----------+----------+------------+-----+------------+--------------------+------------+---+\n",
      "|    true|       [deleted]|                  null|                null|           [deleted]|               0| 1306886400|         null|    0| false|     0|c1x2mqs|t3_hoiup|t1_c1x2mqs|  t3_hoiup|  1427199443|   -1|       false|                 MRR|    t5_2shul| -1|\n",
      "|    true|shr3dthegnarbrah|                  null|                null|It thought the sa...|               0| 1306886400|         null|    0| false|     0|c1x2mqt|t3_hojc7|t1_c1x2mqt|  t3_hojc7|  1427199443|    1|       false|                pics|    t5_2qh0u|  1|\n",
      "|    true|         Decency|               pharaoh|                null|He has tons of co...|               0| 1306886400|         null|    0| false|     0|c1x2mqu|t3_hokrk|t1_c1x2mqu|  t3_hokrk|  1427199443|    5|       false|     HeroesofNewerth|    t5_2r497|  5|\n",
      "|    true|         nathism|                  null|                null|I know exactly ho...|               0| 1306886401|         null|    0| false|     0|c1x2mqv|t3_hojc7|t1_c1x2mqv|  t3_hojc7|  1427199443|    1|       false|                pics|    t5_2qh0u|  1|\n",
      "|    true|       [deleted]|                  null|                null|&gt;Then play sol...|               0| 1306886401|         null|    0| false|     0|c1x2mqw|t3_hog39|t1_c1x2mqw|t1_c1x19qb|  1427199443|   -2|       false|                 tf2|    t5_2qka0| -2|\n",
      "|    true|      DogzOnFire|                  null|                null|Hmm...that's quit...|               0| 1306886401|         null|    0| false|     0|c1x2mqx|t3_ho7lv|t1_c1x2mqx|t1_c1wzps7|  1427199443|    1|       false| fffffffuuuuuuuuuuuu|    t5_2qqlo|  1|\n",
      "|    true|  Neitsyt_Marian|                  NKOR|         North Korea|Ironically I get ...|               0| 1306886402|         null|    0| false|     0|c1x2mqy|t3_ho9t7|t1_c1x2mqy|  t3_ho9t7|  1427199443|    2|       false|              europe|    t5_2qh4j|  2|\n",
      "|    true|        timoleon|                  null|                null|&gt;That was beca...|               0| 1306886402|         null|    0| false|     0|c1x2mqz|t3_ho3zr|t1_c1x2mqz|t1_c1wzmaf|  1427199443|    0|       false|           worldnews|    t5_2qh13|  0|\n",
      "|    true|          Kris18|                  null|                null|Bahahahaha. XD\n",
      "\n",
      "O...|               0| 1306886402|         null|    0| false|     0|c1x2mr0|t3_hohl8|t1_c1x2mr0|  t3_hohl8|  1427199443|   -1|       false| fffffffuuuuuuuuuuuu|    t5_2qqlo| -1|\n",
      "|    true|       [deleted]|                  null|                null|The opinions I've...|               0| 1306886402|         null|    0| false|     0|c1x2mr1|t3_ho06s|t1_c1x2mr1|t1_c1x2ltx|  1427199443|    0|       false|               trees|    t5_2r9vp|  0|\n",
      "|    true|       [deleted]|                  null|                null|           [deleted]|               0| 1306886402|         null|    0| false|     0|c1x2mr2|t3_hom47|t1_c1x2mr2|t1_c1x2kob|  1427199443|    1|       false|           violinist|    t5_2sbn3|  1|\n",
      "|    true|        RubyBlye|                  null|                null|I read a lot of w...|               0| 1306886402|         null|    0| false|     0|c1x2mr3|t3_hoj7a|t1_c1x2mr3|  t3_hoj7a|  1427199443|    2|       false|           AskReddit|    t5_2qh1i|  2|\n",
      "|    true|      SwirlStick|                  null|                null|I know what you m...|               0| 1306886402|         null|    0| false|     0|c1x2mr4|t3_hodqn|t1_c1x2mr4|  t3_hodqn|  1427199443|    1|       false| fffffffuuuuuuuuuuuu|    t5_2qqlo|  1|\n",
      "|    true|       nnnslogan|                  null|                null|A client of mine ...|               0| 1306886403|         null|    0| false|     0|c1x2mr5|t3_hod3f|t1_c1x2mr5|t1_c1x0cer|  1427199443|    3|       false|           AskReddit|    t5_2qh1i|  3|\n",
      "|    true|           Suwop|                  null|                null|But it's not tech...|               0| 1306886403|         null|    0| false|     0|c1x2mr6|t3_hodc3|t1_c1x2mr6|t1_c1x2gep|  1427199443|   -1|       false|           AskReddit|    t5_2qh1i| -1|\n",
      "|    true|        Brimshae|             tftsflair|Tryin' to BS the ...|   Thanks for that. |               0| 1306886404|         null|    0| false|     0|c1x2mr7|t3_hnmm6|t1_c1x2mr7|t1_c1wug90|  1427199443|    1|       false|talesfromtechsupport|    t5_2sfg5|  1|\n",
      "|    true|  EverybodyScram|                  null|                null|&gt;There's not a...|               0| 1306886404|         null|    0| false|     0|c1x2mr8|t3_hofy0|t1_c1x2mr8|t1_c1x2ish|  1427199443|    2|       false|           bicycling|    t5_2qi0s|  2|\n",
      "|    true|        LifeofPI|                  null|                null|Logical and perce...|               0| 1306886404|         null|    0| false|     0|c1x2mr9|t3_hodd3|t1_c1x2mr9|t1_c1x1l35|  1427199443|    1|       false|                news|    t5_2qh3l|  1|\n",
      "|    true|   4th_Dimension|               pertwee|             Pertwee|I've got one too!...|               0| 1306886404|         null|    0| false|     0|c1x2mra|t3_hollm|t1_c1x2mra|  t3_hollm|  1427199443|    6|       false|           doctorwho|    t5_2qhek|  6|\n",
      "|    true|         kaiomai|                  null|                null|Just got there.  ...|               0| 1306886404|         null|    0| false|     0|c1x2mrb|t3_ho4le|t1_c1x2mrb|  t3_ho4le|  1427199443|    2|       false|               Drugs|    t5_2qh7l|  2|\n",
      "+--------+----------------+----------------------+--------------------+--------------------+----------------+-----------+-------------+-----+------+------+-------+--------+----------+----------+------------+-----+------------+--------------------+------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9766511"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 ms, sys: 0 ns, total: 12.4 ms\n",
      "Wall time: 995 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reddit_rdd_2017_01 = df_05.select(\"body\").rdd.flatMap(lambda x: x)\n",
    "reddit_rdd_2017_01.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[deleted]',\n",
       " \"It thought the same thing, searched hard when it seemed like it wasn't going anywhere and now I have four interviews in the next two weeks!\\n\\nKeep on keepin' on broseph!\",\n",
       " \"He has tons of counters in the laning phase:\\n\\n- Vindicator\\n- Demented Shaman\\n- Pharaoh\\n- Silhouette (but honestly, who does she not counter?)\\n- Jeraziah\\n- Tundra\\n- Torturer\\n- Succubus\\n\\netc.\\n\\nLater it's just hard disables and good initiation, Hellflower is incredibly good at shutting him down, any true disabler too.\",\n",
       " \"I know exactly how you feel man.  It's amazing how entry-level positions now require 2-3 years experience and a couple of internships aren't enough.\",\n",
       " \"&gt;Then play soldier or scout.\\n\\nCan't take on a heavy/medic.\",\n",
       " \"Hmm...that's quite a catch of a man you have there. Well done...could you possibly name your house The USS Enterprise or would that be copyright infringement? =P\",\n",
       " \"Ironically I get a lot of my 'major' Euro news from Al-Jazeera.\\n\\nAfter that it's all local or regional.\",\n",
       " \"&gt;That was because the DPRK forces were already exhausted and weren't well equipped anyway. They're much better off this time around.\\n\\nAren't you forgetting a small detail here: some 900,000 Chinese?\\n\\nWithout them, Korea would have been reunited in 1950.\",\n",
       " \"Bahahahaha. XD\\n\\nOne of the best [Fixed]s I've seen.\",\n",
       " 'The opinions I\\'ve expressed relate directly to how I perceive adulthood. \\n\\nWhen I entered the thread I asked a question. The question being \"Are all of you people children?\"\\n\\nThe answer to this is clearly \"No.\" Not every user is a child. I\\'m certainly not one. You sound increasingly like one, but I wouldn\\'t say you are one.\\n\\nThe vast majority of the posters in this subreddit are clearly children. If you want to prove me wrong, you do the survey. The burden of proof is on you at this point, because to me, and every other adult redditor I\\'ve ever associated with who visits this subreddit, it\\'s glaringly obvious that kids run the place.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_rdd = df.select(\"body\").rdd.flatMap(lambda x: x)\n",
    "reddit_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It thought the same thing, searched hard when it seemed like it wasn't going anywhere and now I have four interviews in the next two weeks!\\n\\nKeep on keepin' on broseph!\",\n",
       " \"He has tons of counters in the laning phase:\\n\\n- Vindicator\\n- Demented Shaman\\n- Pharaoh\\n- Silhouette (but honestly, who does she not counter?)\\n- Jeraziah\\n- Tundra\\n- Torturer\\n- Succubus\\n\\netc.\\n\\nLater it's just hard disables and good initiation, Hellflower is incredibly good at shutting him down, any true disabler too.\",\n",
       " \"I know exactly how you feel man.  It's amazing how entry-level positions now require 2-3 years experience and a couple of internships aren't enough.\",\n",
       " \"&gt;Then play soldier or scout.\\n\\nCan't take on a heavy/medic.\",\n",
       " \"Hmm...that's quite a catch of a man you have there. Well done...could you possibly name your house The USS Enterprise or would that be copyright infringement? =P\",\n",
       " \"Ironically I get a lot of my 'major' Euro news from Al-Jazeera.\\n\\nAfter that it's all local or regional.\",\n",
       " \"&gt;That was because the DPRK forces were already exhausted and weren't well equipped anyway. They're much better off this time around.\\n\\nAren't you forgetting a small detail here: some 900,000 Chinese?\\n\\nWithout them, Korea would have been reunited in 1950.\",\n",
       " \"Bahahahaha. XD\\n\\nOne of the best [Fixed]s I've seen.\",\n",
       " 'The opinions I\\'ve expressed relate directly to how I perceive adulthood. \\n\\nWhen I entered the thread I asked a question. The question being \"Are all of you people children?\"\\n\\nThe answer to this is clearly \"No.\" Not every user is a child. I\\'m certainly not one. You sound increasingly like one, but I wouldn\\'t say you are one.\\n\\nThe vast majority of the posters in this subreddit are clearly children. If you want to prove me wrong, you do the survey. The burden of proof is on you at this point, because to me, and every other adult redditor I\\'ve ever associated with who visits this subreddit, it\\'s glaringly obvious that kids run the place.',\n",
       " 'I read a lot of words before I heard them pronounced and so I would often make a fucks pass with words like hyper bowl, or fake aid.\\n\\n\\n\\n\\n\\n(faux pas, hyperbole, facade)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_rdd2 = reddit_rdd.filter(lambda row: row != \"[deleted]\")\n",
    "reddit_rdd2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it',\n",
       "  'thought',\n",
       "  'the',\n",
       "  'same',\n",
       "  'thing',\n",
       "  'searched',\n",
       "  'hard',\n",
       "  'when',\n",
       "  'it',\n",
       "  'seemed',\n",
       "  'like',\n",
       "  'it',\n",
       "  'wasn',\n",
       "  't',\n",
       "  'going',\n",
       "  'anywhere',\n",
       "  'and',\n",
       "  'now',\n",
       "  'i',\n",
       "  'have',\n",
       "  'four',\n",
       "  'interviews',\n",
       "  'in',\n",
       "  'the',\n",
       "  'next',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  'keep',\n",
       "  'on',\n",
       "  'keepin',\n",
       "  'on',\n",
       "  'broseph'],\n",
       " ['he',\n",
       "  'has',\n",
       "  'tons',\n",
       "  'of',\n",
       "  'counters',\n",
       "  'in',\n",
       "  'the',\n",
       "  'laning',\n",
       "  'phase',\n",
       "  'vindicator',\n",
       "  'demented',\n",
       "  'shaman',\n",
       "  'pharaoh',\n",
       "  'silhouette',\n",
       "  'but',\n",
       "  'honestly',\n",
       "  'who',\n",
       "  'does',\n",
       "  'she',\n",
       "  'not',\n",
       "  'counter',\n",
       "  'jeraziah',\n",
       "  'tundra',\n",
       "  'torturer',\n",
       "  'succubus',\n",
       "  'etc',\n",
       "  'later',\n",
       "  'it',\n",
       "  's',\n",
       "  'just',\n",
       "  'hard',\n",
       "  'disables',\n",
       "  'and',\n",
       "  'good',\n",
       "  'initiation',\n",
       "  'hellflower',\n",
       "  'is',\n",
       "  'incredibly',\n",
       "  'good',\n",
       "  'at',\n",
       "  'shutting',\n",
       "  'him',\n",
       "  'down',\n",
       "  'any',\n",
       "  'true',\n",
       "  'disabler',\n",
       "  'too'],\n",
       " ['i',\n",
       "  'know',\n",
       "  'exactly',\n",
       "  'how',\n",
       "  'you',\n",
       "  'feel',\n",
       "  'man',\n",
       "  'it',\n",
       "  's',\n",
       "  'amazing',\n",
       "  'how',\n",
       "  'entry',\n",
       "  'level',\n",
       "  'positions',\n",
       "  'now',\n",
       "  'require',\n",
       "  'years',\n",
       "  'experience',\n",
       "  'and',\n",
       "  'a',\n",
       "  'couple',\n",
       "  'of',\n",
       "  'internships',\n",
       "  'aren',\n",
       "  't',\n",
       "  'enough'],\n",
       " ['gt',\n",
       "  'then',\n",
       "  'play',\n",
       "  'soldier',\n",
       "  'or',\n",
       "  'scout',\n",
       "  'can',\n",
       "  't',\n",
       "  'take',\n",
       "  'on',\n",
       "  'a',\n",
       "  'heavy',\n",
       "  'medic'],\n",
       " ['hmm',\n",
       "  'that',\n",
       "  's',\n",
       "  'quite',\n",
       "  'a',\n",
       "  'catch',\n",
       "  'of',\n",
       "  'a',\n",
       "  'man',\n",
       "  'you',\n",
       "  'have',\n",
       "  'there',\n",
       "  'well',\n",
       "  'done',\n",
       "  'could',\n",
       "  'you',\n",
       "  'possibly',\n",
       "  'name',\n",
       "  'your',\n",
       "  'house',\n",
       "  'the',\n",
       "  'uss',\n",
       "  'enterprise',\n",
       "  'or',\n",
       "  'would',\n",
       "  'that',\n",
       "  'be',\n",
       "  'copyright',\n",
       "  'infringement',\n",
       "  'p'],\n",
       " ['ironically',\n",
       "  'i',\n",
       "  'get',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'my',\n",
       "  'major',\n",
       "  'euro',\n",
       "  'news',\n",
       "  'from',\n",
       "  'al',\n",
       "  'jazeera',\n",
       "  'after',\n",
       "  'that',\n",
       "  'it',\n",
       "  's',\n",
       "  'all',\n",
       "  'local',\n",
       "  'or',\n",
       "  'regional'],\n",
       " ['gt',\n",
       "  'that',\n",
       "  'was',\n",
       "  'because',\n",
       "  'the',\n",
       "  'dprk',\n",
       "  'forces',\n",
       "  'were',\n",
       "  'already',\n",
       "  'exhausted',\n",
       "  'and',\n",
       "  'weren',\n",
       "  't',\n",
       "  'well',\n",
       "  'equipped',\n",
       "  'anyway',\n",
       "  'they',\n",
       "  're',\n",
       "  'much',\n",
       "  'better',\n",
       "  'off',\n",
       "  'this',\n",
       "  'time',\n",
       "  'around',\n",
       "  'aren',\n",
       "  't',\n",
       "  'you',\n",
       "  'forgetting',\n",
       "  'a',\n",
       "  'small',\n",
       "  'detail',\n",
       "  'here',\n",
       "  'some',\n",
       "  'chinese',\n",
       "  'without',\n",
       "  'them',\n",
       "  'korea',\n",
       "  'would',\n",
       "  'have',\n",
       "  'been',\n",
       "  'reunited',\n",
       "  'in'],\n",
       " ['bahahahaha',\n",
       "  'xd',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'best',\n",
       "  'fixed',\n",
       "  's',\n",
       "  'i',\n",
       "  've',\n",
       "  'seen'],\n",
       " ['the',\n",
       "  'opinions',\n",
       "  'i',\n",
       "  've',\n",
       "  'expressed',\n",
       "  'relate',\n",
       "  'directly',\n",
       "  'to',\n",
       "  'how',\n",
       "  'i',\n",
       "  'perceive',\n",
       "  'adulthood',\n",
       "  'when',\n",
       "  'i',\n",
       "  'entered',\n",
       "  'the',\n",
       "  'thread',\n",
       "  'i',\n",
       "  'asked',\n",
       "  'a',\n",
       "  'question',\n",
       "  'the',\n",
       "  'question',\n",
       "  'being',\n",
       "  'are',\n",
       "  'all',\n",
       "  'of',\n",
       "  'you',\n",
       "  'people',\n",
       "  'children',\n",
       "  'the',\n",
       "  'answer',\n",
       "  'to',\n",
       "  'this',\n",
       "  'is',\n",
       "  'clearly',\n",
       "  'no',\n",
       "  'not',\n",
       "  'every',\n",
       "  'user',\n",
       "  'is',\n",
       "  'a',\n",
       "  'child',\n",
       "  'i',\n",
       "  'm',\n",
       "  'certainly',\n",
       "  'not',\n",
       "  'one',\n",
       "  'you',\n",
       "  'sound',\n",
       "  'increasingly',\n",
       "  'like',\n",
       "  'one',\n",
       "  'but',\n",
       "  'i',\n",
       "  'wouldn',\n",
       "  't',\n",
       "  'say',\n",
       "  'you',\n",
       "  'are',\n",
       "  'one',\n",
       "  'the',\n",
       "  'vast',\n",
       "  'majority',\n",
       "  'of',\n",
       "  'the',\n",
       "  'posters',\n",
       "  'in',\n",
       "  'this',\n",
       "  'subreddit',\n",
       "  'are',\n",
       "  'clearly',\n",
       "  'children',\n",
       "  'if',\n",
       "  'you',\n",
       "  'want',\n",
       "  'to',\n",
       "  'prove',\n",
       "  'me',\n",
       "  'wrong',\n",
       "  'you',\n",
       "  'do',\n",
       "  'the',\n",
       "  'survey',\n",
       "  'the',\n",
       "  'burden',\n",
       "  'of',\n",
       "  'proof',\n",
       "  'is',\n",
       "  'on',\n",
       "  'you',\n",
       "  'at',\n",
       "  'this',\n",
       "  'point',\n",
       "  'because',\n",
       "  'to',\n",
       "  'me',\n",
       "  'and',\n",
       "  'every',\n",
       "  'other',\n",
       "  'adult',\n",
       "  'redditor',\n",
       "  'i',\n",
       "  've',\n",
       "  'ever',\n",
       "  'associated',\n",
       "  'with',\n",
       "  'who',\n",
       "  'visits',\n",
       "  'this',\n",
       "  'subreddit',\n",
       "  'it',\n",
       "  's',\n",
       "  'glaringly',\n",
       "  'obvious',\n",
       "  'that',\n",
       "  'kids',\n",
       "  'run',\n",
       "  'the',\n",
       "  'place'],\n",
       " ['i',\n",
       "  'read',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'words',\n",
       "  'before',\n",
       "  'i',\n",
       "  'heard',\n",
       "  'them',\n",
       "  'pronounced',\n",
       "  'and',\n",
       "  'so',\n",
       "  'i',\n",
       "  'would',\n",
       "  'often',\n",
       "  'make',\n",
       "  'a',\n",
       "  'fucks',\n",
       "  'pass',\n",
       "  'with',\n",
       "  'words',\n",
       "  'like',\n",
       "  'hyper',\n",
       "  'bowl',\n",
       "  'or',\n",
       "  'fake',\n",
       "  'aid',\n",
       "  'faux',\n",
       "  'pas',\n",
       "  'hyperbole',\n",
       "  'facade']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def lower_rdd(lines):\n",
    "      lines = lines.lower()\n",
    "      words = re.findall(r'[a-zA-Z]+', lines)\n",
    "      return words\n",
    "reddit_words = reddit_rdd2.map(lower_rdd)\n",
    "reddit_words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "stopword_list.append('http')\n",
    "stopword_list.append('com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWordsFunct(x):\n",
    "    filteredSentence = [w for w in x if not w in stopword_list]\n",
    "    return filteredSentence\n",
    "stopword_reddit = reddit_words.map(removeStopWordsFunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thought',\n",
       "  'thing',\n",
       "  'searched',\n",
       "  'hard',\n",
       "  'seemed',\n",
       "  'like',\n",
       "  'going',\n",
       "  'anywhere',\n",
       "  'four',\n",
       "  'interviews',\n",
       "  'next',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  'keep',\n",
       "  'keepin',\n",
       "  'broseph'],\n",
       " ['tons',\n",
       "  'counters',\n",
       "  'laning',\n",
       "  'phase',\n",
       "  'vindicator',\n",
       "  'demented',\n",
       "  'shaman',\n",
       "  'pharaoh',\n",
       "  'silhouette',\n",
       "  'honestly',\n",
       "  'counter',\n",
       "  'jeraziah',\n",
       "  'tundra',\n",
       "  'torturer',\n",
       "  'succubus',\n",
       "  'etc',\n",
       "  'later',\n",
       "  'hard',\n",
       "  'disables',\n",
       "  'good',\n",
       "  'initiation',\n",
       "  'hellflower',\n",
       "  'incredibly',\n",
       "  'good',\n",
       "  'shutting',\n",
       "  'true',\n",
       "  'disabler'],\n",
       " ['know',\n",
       "  'exactly',\n",
       "  'feel',\n",
       "  'man',\n",
       "  'amazing',\n",
       "  'entry',\n",
       "  'level',\n",
       "  'positions',\n",
       "  'require',\n",
       "  'years',\n",
       "  'experience',\n",
       "  'couple',\n",
       "  'internships',\n",
       "  'enough'],\n",
       " ['gt', 'play', 'soldier', 'scout', 'take', 'heavy', 'medic'],\n",
       " ['hmm',\n",
       "  'quite',\n",
       "  'catch',\n",
       "  'man',\n",
       "  'well',\n",
       "  'done',\n",
       "  'could',\n",
       "  'possibly',\n",
       "  'name',\n",
       "  'house',\n",
       "  'uss',\n",
       "  'enterprise',\n",
       "  'would',\n",
       "  'copyright',\n",
       "  'infringement',\n",
       "  'p'],\n",
       " ['ironically',\n",
       "  'get',\n",
       "  'lot',\n",
       "  'major',\n",
       "  'euro',\n",
       "  'news',\n",
       "  'al',\n",
       "  'jazeera',\n",
       "  'local',\n",
       "  'regional'],\n",
       " ['gt',\n",
       "  'dprk',\n",
       "  'forces',\n",
       "  'already',\n",
       "  'exhausted',\n",
       "  'well',\n",
       "  'equipped',\n",
       "  'anyway',\n",
       "  'much',\n",
       "  'better',\n",
       "  'time',\n",
       "  'around',\n",
       "  'forgetting',\n",
       "  'small',\n",
       "  'detail',\n",
       "  'chinese',\n",
       "  'without',\n",
       "  'korea',\n",
       "  'would',\n",
       "  'reunited'],\n",
       " ['bahahahaha', 'xd', 'one', 'best', 'fixed', 'seen'],\n",
       " ['opinions',\n",
       "  'expressed',\n",
       "  'relate',\n",
       "  'directly',\n",
       "  'perceive',\n",
       "  'adulthood',\n",
       "  'entered',\n",
       "  'thread',\n",
       "  'asked',\n",
       "  'question',\n",
       "  'question',\n",
       "  'people',\n",
       "  'children',\n",
       "  'answer',\n",
       "  'clearly',\n",
       "  'every',\n",
       "  'user',\n",
       "  'child',\n",
       "  'certainly',\n",
       "  'one',\n",
       "  'sound',\n",
       "  'increasingly',\n",
       "  'like',\n",
       "  'one',\n",
       "  'say',\n",
       "  'one',\n",
       "  'vast',\n",
       "  'majority',\n",
       "  'posters',\n",
       "  'subreddit',\n",
       "  'clearly',\n",
       "  'children',\n",
       "  'want',\n",
       "  'prove',\n",
       "  'wrong',\n",
       "  'survey',\n",
       "  'burden',\n",
       "  'proof',\n",
       "  'point',\n",
       "  'every',\n",
       "  'adult',\n",
       "  'redditor',\n",
       "  'ever',\n",
       "  'associated',\n",
       "  'visits',\n",
       "  'subreddit',\n",
       "  'glaringly',\n",
       "  'obvious',\n",
       "  'kids',\n",
       "  'run',\n",
       "  'place'],\n",
       " ['read',\n",
       "  'lot',\n",
       "  'words',\n",
       "  'heard',\n",
       "  'pronounced',\n",
       "  'would',\n",
       "  'often',\n",
       "  'make',\n",
       "  'fucks',\n",
       "  'pass',\n",
       "  'words',\n",
       "  'like',\n",
       "  'hyper',\n",
       "  'bowl',\n",
       "  'fake',\n",
       "  'aid',\n",
       "  'faux',\n",
       "  'pas',\n",
       "  'hyperbole',\n",
       "  'facade']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_reddit.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(lines):\n",
    "    counts = lines.flatMap(lambda line: line) \\\n",
    "        .map(lambda word: (word, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \n",
    "    return counts.sortBy(lambda a: a[1], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('like', 1336082),\n",
       " ('would', 1007222),\n",
       " ('one', 972899),\n",
       " ('people', 968006),\n",
       " ('get', 919777),\n",
       " ('think', 770083),\n",
       " ('know', 635489),\n",
       " ('time', 630918),\n",
       " ('really', 615540),\n",
       " ('good', 588170),\n",
       " ('gt', 513810),\n",
       " ('also', 496955),\n",
       " ('much', 494461),\n",
       " ('make', 487558),\n",
       " ('even', 480145),\n",
       " ('see', 463930),\n",
       " ('go', 454549),\n",
       " ('well', 453212),\n",
       " ('way', 452084),\n",
       " ('want', 445451),\n",
       " ('www', 433445),\n",
       " ('could', 427470),\n",
       " ('going', 394079),\n",
       " ('something', 390411),\n",
       " ('right', 390294),\n",
       " ('say', 377506),\n",
       " ('still', 375270),\n",
       " ('never', 350001),\n",
       " ('first', 328446),\n",
       " ('got', 323671),\n",
       " ('thing', 321154),\n",
       " ('actually', 320406),\n",
       " ('work', 318130),\n",
       " ('amp', 317455),\n",
       " ('back', 317261),\n",
       " ('need', 309452),\n",
       " ('use', 306063),\n",
       " ('things', 304527),\n",
       " ('pretty', 303039),\n",
       " ('though', 298917),\n",
       " ('sure', 296629),\n",
       " ('better', 288585),\n",
       " ('take', 288355),\n",
       " ('someone', 286832),\n",
       " ('lot', 280337),\n",
       " ('us', 279557),\n",
       " ('game', 270954),\n",
       " ('said', 256334),\n",
       " ('many', 253553),\n",
       " ('day', 250207),\n",
       " ('years', 250041),\n",
       " ('every', 249231),\n",
       " ('look', 248868),\n",
       " ('find', 246506),\n",
       " ('around', 245975),\n",
       " ('always', 242922),\n",
       " ('point', 242367),\n",
       " ('love', 241446),\n",
       " ('probably', 240421),\n",
       " ('new', 239056),\n",
       " ('reddit', 236464),\n",
       " ('great', 229423),\n",
       " ('anything', 228575),\n",
       " ('two', 226482),\n",
       " ('shit', 226224),\n",
       " ('made', 225595),\n",
       " ('feel', 223939),\n",
       " ('little', 218358),\n",
       " ('ever', 215946),\n",
       " ('life', 215067),\n",
       " ('best', 214398),\n",
       " ('try', 211396),\n",
       " ('guy', 210615),\n",
       " ('bad', 209422),\n",
       " ('r', 206884),\n",
       " ('yes', 205169),\n",
       " ('watch', 204108),\n",
       " ('might', 201946),\n",
       " ('man', 201936),\n",
       " ('long', 200492),\n",
       " ('used', 199991),\n",
       " ('fuck', 196174),\n",
       " ('thought', 191973),\n",
       " ('yeah', 191864),\n",
       " ('enough', 188727),\n",
       " ('give', 188039),\n",
       " ('money', 188013),\n",
       " ('maybe', 187616),\n",
       " ('read', 186324),\n",
       " ('getting', 185929),\n",
       " ('since', 185529),\n",
       " ('mean', 182213),\n",
       " ('may', 178937),\n",
       " ('put', 178343),\n",
       " ('without', 177740),\n",
       " ('thanks', 177669),\n",
       " ('makes', 176504),\n",
       " ('different', 175011),\n",
       " ('let', 174647),\n",
       " ('person', 172151),\n",
       " ('year', 171917),\n",
       " ('world', 169448),\n",
       " ('post', 166707),\n",
       " ('fucking', 165797),\n",
       " ('come', 165245),\n",
       " ('old', 165212),\n",
       " ('last', 164967),\n",
       " ('keep', 164913),\n",
       " ('oh', 164825),\n",
       " ('everyone', 163749),\n",
       " ('another', 163415),\n",
       " ('believe', 163395),\n",
       " ('nothing', 161157),\n",
       " ('part', 159986),\n",
       " ('play', 159389),\n",
       " ('problem', 158687),\n",
       " ('trying', 158191),\n",
       " ('god', 157981),\n",
       " ('tell', 157078),\n",
       " ('bit', 155788),\n",
       " ('least', 155627),\n",
       " ('wrong', 154666),\n",
       " ('kind', 150594),\n",
       " ('v', 150110),\n",
       " ('hard', 148867),\n",
       " ('anyone', 148123),\n",
       " ('else', 147998),\n",
       " ('saying', 147633),\n",
       " ('big', 147113),\n",
       " ('less', 146575),\n",
       " ('nice', 146381),\n",
       " ('idea', 145701),\n",
       " ('end', 144789),\n",
       " ('help', 144276),\n",
       " ('reason', 144085),\n",
       " ('youtube', 142748),\n",
       " ('done', 141879),\n",
       " ('everything', 140169),\n",
       " ('far', 139946),\n",
       " ('high', 139013),\n",
       " ('either', 138768),\n",
       " ('live', 138715),\n",
       " ('place', 138669),\n",
       " ('stuff', 137747),\n",
       " ('real', 137492),\n",
       " ('away', 137243),\n",
       " ('fact', 135414),\n",
       " ('games', 133533),\n",
       " ('awesome', 132814),\n",
       " ('free', 132811),\n",
       " ('seems', 132525),\n",
       " ('start', 131891),\n",
       " ('times', 131766),\n",
       " ('etc', 131319),\n",
       " ('friends', 131136),\n",
       " ('making', 130308),\n",
       " ('already', 129739),\n",
       " ('using', 128939),\n",
       " ('looks', 128829),\n",
       " ('looking', 128695),\n",
       " ('show', 126607),\n",
       " ('next', 125990),\n",
       " ('job', 125611),\n",
       " ('true', 123731),\n",
       " ('understand', 123209),\n",
       " ('seen', 122826),\n",
       " ('able', 122512),\n",
       " ('school', 122469),\n",
       " ('agree', 121510),\n",
       " ('org', 121365),\n",
       " ('edit', 121303),\n",
       " ('stop', 118751),\n",
       " ('went', 118471),\n",
       " ('guess', 116940),\n",
       " ('buy', 116762),\n",
       " ('friend', 116118),\n",
       " ('whole', 116096),\n",
       " ('women', 115831),\n",
       " ('guys', 115125),\n",
       " ('case', 113271),\n",
       " ('call', 109928),\n",
       " ('care', 108684),\n",
       " ('remember', 108203),\n",
       " ('pay', 107847),\n",
       " ('mind', 107315),\n",
       " ('found', 106969),\n",
       " ('name', 106071),\n",
       " ('quite', 105001),\n",
       " ('system', 104272),\n",
       " ('fun', 104253),\n",
       " ('video', 103991),\n",
       " ('change', 103901),\n",
       " ('however', 103833),\n",
       " ('talking', 103098),\n",
       " ('instead', 103029),\n",
       " ('run', 102842),\n",
       " ('sorry', 102807),\n",
       " ('question', 102720),\n",
       " ('ago', 102453),\n",
       " ('en', 101807),\n",
       " ('home', 101594),\n",
       " ('yet', 100794),\n",
       " ('usually', 100705),\n",
       " ('c', 100694),\n",
       " ('means', 100237),\n",
       " ('almost', 99390),\n",
       " ('story', 98772),\n",
       " ('must', 97943),\n",
       " ('exactly', 97895),\n",
       " ('days', 97807),\n",
       " ('comments', 97680),\n",
       " ('rather', 97492),\n",
       " ('e', 97489),\n",
       " ('ask', 97384),\n",
       " ('car', 97330),\n",
       " ('government', 96857),\n",
       " ('girl', 96313),\n",
       " ('x', 96236),\n",
       " ('hope', 96010),\n",
       " ('sex', 95760),\n",
       " ('hate', 95391),\n",
       " ('left', 95279),\n",
       " ('gets', 93738),\n",
       " ('jpg', 93603),\n",
       " ('definitely', 93411),\n",
       " ('course', 93275),\n",
       " ('sounds', 93268),\n",
       " ('came', 92747),\n",
       " ('please', 92531),\n",
       " ('top', 92461),\n",
       " ('cool', 92260),\n",
       " ('started', 91963),\n",
       " ('sense', 90859),\n",
       " ('wikipedia', 90575),\n",
       " ('state', 90048),\n",
       " ('completely', 89681),\n",
       " ('full', 89467),\n",
       " ('thank', 89332),\n",
       " ('second', 89236),\n",
       " ('hell', 88665),\n",
       " ('lol', 88425),\n",
       " ('small', 88291),\n",
       " ('side', 88286),\n",
       " ('wiki', 88271),\n",
       " ('food', 88045),\n",
       " ('p', 88039),\n",
       " ('black', 87951),\n",
       " ('kids', 87710),\n",
       " ('thinking', 86948),\n",
       " ('called', 86932),\n",
       " ('men', 86796),\n",
       " ('matter', 86759),\n",
       " ('heard', 86341),\n",
       " ('seem', 86239),\n",
       " ('night', 86157),\n",
       " ('house', 86144),\n",
       " ('experience', 85582),\n",
       " ('wait', 85468),\n",
       " ('set', 85283),\n",
       " ('water', 85162),\n",
       " ('comment', 84568),\n",
       " ('b', 84436),\n",
       " ('power', 84383),\n",
       " ('link', 84280),\n",
       " ('others', 84100),\n",
       " ('imgur', 84079),\n",
       " ('check', 83775),\n",
       " ('saw', 83682),\n",
       " ('playing', 83594),\n",
       " ('head', 83560),\n",
       " ('works', 83300),\n",
       " ('week', 82717),\n",
       " ('says', 82223),\n",
       " ('often', 82143),\n",
       " ('sometimes', 82073),\n",
       " ('took', 81495),\n",
       " ('wanted', 81395),\n",
       " ('talk', 81315),\n",
       " ('face', 81204),\n",
       " ('comes', 80683),\n",
       " ('working', 80639),\n",
       " ('hand', 80112),\n",
       " ('especially', 80089),\n",
       " ('whatever', 80004),\n",
       " ('half', 79691),\n",
       " ('seriously', 79690),\n",
       " ('book', 79441),\n",
       " ('support', 78779),\n",
       " ('worth', 78738),\n",
       " ('possible', 78367),\n",
       " ('stupid', 77687),\n",
       " ('based', 77558),\n",
       " ('family', 77319),\n",
       " ('couple', 77121),\n",
       " ('told', 76966),\n",
       " ('country', 76710),\n",
       " ('fine', 76644),\n",
       " ('internet', 76514),\n",
       " ('funny', 76096),\n",
       " ('example', 76095),\n",
       " ('goes', 76017),\n",
       " ('unless', 75847),\n",
       " ('happen', 75481),\n",
       " ('damn', 75372),\n",
       " ('easy', 75343),\n",
       " ('word', 75286),\n",
       " ('ones', 75118),\n",
       " ('hours', 74919),\n",
       " ('open', 74318),\n",
       " ('movie', 74195),\n",
       " ('sort', 73666),\n",
       " ('months', 73543),\n",
       " ('simply', 73541),\n",
       " ('level', 73491),\n",
       " ('number', 73475),\n",
       " ('likely', 73422),\n",
       " ('taking', 72590),\n",
       " ('hit', 72588),\n",
       " ('move', 72447),\n",
       " ('happy', 72291),\n",
       " ('line', 72212),\n",
       " ('tried', 72167),\n",
       " ('later', 72015),\n",
       " ('issue', 71996),\n",
       " ('ass', 71699),\n",
       " ('interesting', 71524),\n",
       " ('eat', 70455),\n",
       " ('single', 70432),\n",
       " ('needs', 70374),\n",
       " ('white', 70233),\n",
       " ('ok', 70163),\n",
       " ('deal', 69976),\n",
       " ('music', 69555),\n",
       " ('non', 69137),\n",
       " ('hear', 69008),\n",
       " ('body', 68499),\n",
       " ('happened', 68472),\n",
       " ('wow', 68172),\n",
       " ('picture', 68134),\n",
       " ('anyway', 68065),\n",
       " ('google', 67866),\n",
       " ('team', 67271),\n",
       " ('close', 67066),\n",
       " ('law', 67018),\n",
       " ('human', 66976),\n",
       " ('turn', 66588),\n",
       " ('sound', 66000),\n",
       " ('leave', 65828),\n",
       " ('article', 65729),\n",
       " ('reading', 65459),\n",
       " ('important', 65143),\n",
       " ('today', 65022),\n",
       " ('similar', 64970),\n",
       " ('kid', 64935),\n",
       " ('k', 64915),\n",
       " ('woman', 64854),\n",
       " ('war', 64487),\n",
       " ('wish', 64163),\n",
       " ('answer', 63884),\n",
       " ('minutes', 63622),\n",
       " ('f', 63570),\n",
       " ('huge', 63506),\n",
       " ('three', 63303),\n",
       " ('public', 63241),\n",
       " ('amazing', 63183),\n",
       " ('together', 63013),\n",
       " ('basically', 62922),\n",
       " ('learn', 62888),\n",
       " ('gay', 62600),\n",
       " ('cause', 62554),\n",
       " ('american', 62528),\n",
       " ('order', 62371),\n",
       " ('add', 62196),\n",
       " ('front', 62042),\n",
       " ('u', 62027),\n",
       " ('rest', 61921),\n",
       " ('phone', 61693),\n",
       " ('type', 61560),\n",
       " ('parents', 61368),\n",
       " ('news', 61251),\n",
       " ('dude', 61083),\n",
       " ('coming', 61083),\n",
       " ('class', 61002),\n",
       " ('running', 60842),\n",
       " ('difference', 60799),\n",
       " ('mine', 60775),\n",
       " ('self', 60562),\n",
       " ('business', 60554),\n",
       " ('html', 60547),\n",
       " ('situation', 60441),\n",
       " ('opinion', 60258),\n",
       " ('past', 60218),\n",
       " ('totally', 59904),\n",
       " ('large', 59678),\n",
       " ('entire', 59440),\n",
       " ('amount', 59435),\n",
       " ('kill', 59373),\n",
       " ('group', 59366),\n",
       " ('played', 59338),\n",
       " ('argument', 59285),\n",
       " ('drive', 58914),\n",
       " ('control', 58681),\n",
       " ('given', 58569),\n",
       " ('enjoy', 58500),\n",
       " ('room', 58457),\n",
       " ('thread', 57917),\n",
       " ('child', 57709),\n",
       " ('children', 57587),\n",
       " ('city', 57488),\n",
       " ('company', 57433),\n",
       " ('dont', 57418),\n",
       " ('lost', 57126),\n",
       " ('whether', 56817),\n",
       " ('area', 56782),\n",
       " ('become', 56609),\n",
       " ('problems', 56496),\n",
       " ('living', 56365),\n",
       " ('op', 56312),\n",
       " ('girls', 56284),\n",
       " ('absolutely', 56207),\n",
       " ('obviously', 56184),\n",
       " ('original', 56029),\n",
       " ('hey', 55965),\n",
       " ('hot', 55950),\n",
       " ('list', 55939),\n",
       " ('dog', 55912),\n",
       " ('happens', 55751),\n",
       " ('religion', 55542),\n",
       " ('page', 55388),\n",
       " ('cut', 55235),\n",
       " ('college', 55039),\n",
       " ('light', 54677),\n",
       " ('party', 54575),\n",
       " ('g', 54494),\n",
       " ('low', 54448),\n",
       " ('words', 54440),\n",
       " ('crazy', 54143),\n",
       " ('except', 54139),\n",
       " ('certain', 53657),\n",
       " ('general', 53518),\n",
       " ('knew', 53512),\n",
       " ('although', 53346),\n",
       " ('outside', 53183),\n",
       " ('behind', 53004),\n",
       " ('takes', 52935),\n",
       " ('upvote', 52931),\n",
       " ('month', 52842),\n",
       " ('computer', 52606),\n",
       " ('worked', 52383),\n",
       " ('wants', 52227),\n",
       " ('store', 51671),\n",
       " ('history', 51533),\n",
       " ('information', 51501),\n",
       " ('police', 51453),\n",
       " ('cannot', 51451),\n",
       " ('along', 51426),\n",
       " ('watching', 51389),\n",
       " ('cost', 51162),\n",
       " ('due', 51141),\n",
       " ('personal', 51140),\n",
       " ('im', 51022),\n",
       " ('science', 50987),\n",
       " ('short', 50611),\n",
       " ('haha', 50554),\n",
       " ('asked', 50433),\n",
       " ('imagine', 50320),\n",
       " ('evidence', 50262),\n",
       " ('common', 50161),\n",
       " ('th', 50095),\n",
       " ('source', 50085),\n",
       " ('honestly', 50020),\n",
       " ('poor', 49833),\n",
       " ('realize', 49819),\n",
       " ('worse', 49717),\n",
       " ('death', 49695),\n",
       " ('alone', 49590),\n",
       " ('okay', 49469),\n",
       " ('tv', 49445),\n",
       " ('n', 49269),\n",
       " ('price', 49230),\n",
       " ('site', 49194),\n",
       " ('issues', 49170),\n",
       " ('version', 49146),\n",
       " ('simple', 49108),\n",
       " ('stay', 49071),\n",
       " ('favorite', 49062),\n",
       " ('consider', 48998),\n",
       " ('seeing', 48797),\n",
       " ('books', 48749),\n",
       " ('player', 48744),\n",
       " ('soon', 48730),\n",
       " ('chance', 48482),\n",
       " ('relationship', 48212),\n",
       " ('personally', 48184),\n",
       " ('market', 48082),\n",
       " ('states', 48028),\n",
       " ('middle', 48010),\n",
       " ('age', 47976),\n",
       " ('perhaps', 47859),\n",
       " ('social', 47782),\n",
       " ('red', 47777),\n",
       " ('normal', 47724),\n",
       " ('pick', 47687),\n",
       " ('spend', 47624),\n",
       " ('super', 47355),\n",
       " ('win', 47257),\n",
       " ('dead', 47203),\n",
       " ('account', 47139),\n",
       " ('actual', 47125),\n",
       " ('bring', 47049),\n",
       " ('future', 46999),\n",
       " ('weeks', 46945),\n",
       " ('interested', 46820),\n",
       " ('longer', 46781),\n",
       " ('gonna', 46696),\n",
       " ('fat', 46540),\n",
       " ('correct', 46532),\n",
       " ('posted', 46320),\n",
       " ('choice', 46292),\n",
       " ('joke', 46250),\n",
       " ('fair', 46210),\n",
       " ('certainly', 46080),\n",
       " ('break', 46062),\n",
       " ('bought', 45880),\n",
       " ('local', 45870),\n",
       " ('rights', 45839),\n",
       " ('mostly', 45700),\n",
       " ('wife', 45643),\n",
       " ('america', 45543),\n",
       " ('meant', 45460),\n",
       " ('taken', 45290),\n",
       " ('religious', 45242),\n",
       " ('die', 45164),\n",
       " ('luck', 45096),\n",
       " ('mom', 45079),\n",
       " ('society', 45032),\n",
       " ('content', 44983),\n",
       " ('looked', 44961),\n",
       " ('act', 44951),\n",
       " ('hold', 44743),\n",
       " ('gave', 44533),\n",
       " ('lots', 44524),\n",
       " ('asking', 44471),\n",
       " ('advice', 44458),\n",
       " ('felt', 44436),\n",
       " ('hour', 44418),\n",
       " ('series', 44362),\n",
       " ('giving', 44333),\n",
       " ('near', 44327),\n",
       " ('serious', 44322),\n",
       " ('dad', 44306),\n",
       " ('online', 44211),\n",
       " ('space', 44156),\n",
       " ('form', 43878),\n",
       " ('current', 43868),\n",
       " ('straight', 43769),\n",
       " ('per', 43700),\n",
       " ('drink', 43699),\n",
       " ('weight', 43646),\n",
       " ('generally', 43627),\n",
       " ('players', 43615),\n",
       " ('view', 43515),\n",
       " ('thats', 43453),\n",
       " ('feeling', 43416),\n",
       " ('knows', 43383),\n",
       " ('within', 43256),\n",
       " ('fast', 43255),\n",
       " ('build', 43218),\n",
       " ('easily', 43203),\n",
       " ('shot', 43172),\n",
       " ('otherwise', 43040),\n",
       " ('card', 43000),\n",
       " ('several', 42975),\n",
       " ('lose', 42960),\n",
       " ('community', 42923),\n",
       " ('term', 42720),\n",
       " ('perfect', 42637),\n",
       " ('weird', 42635),\n",
       " ('explain', 42458),\n",
       " ('search', 42353),\n",
       " ('higher', 42256),\n",
       " ('clear', 42251),\n",
       " ('service', 42215),\n",
       " ('easier', 42204),\n",
       " ('hair', 42156),\n",
       " ('early', 42061),\n",
       " ('song', 42024),\n",
       " ('bunch', 41871),\n",
       " ('quality', 41671),\n",
       " ('walk', 41666),\n",
       " ('force', 41608),\n",
       " ('points', 41374),\n",
       " ('clearly', 41312),\n",
       " ('young', 41296),\n",
       " ('plus', 41231),\n",
       " ('save', 41178),\n",
       " ('questions', 41138),\n",
       " ('beer', 41068),\n",
       " ('stand', 41062),\n",
       " ('cheap', 40947),\n",
       " ('fan', 40878),\n",
       " ('paid', 40833),\n",
       " ('expect', 40828),\n",
       " ('title', 40669),\n",
       " ('fire', 40612),\n",
       " ('shows', 40552),\n",
       " ('needed', 40542),\n",
       " ('extra', 40522),\n",
       " ('figure', 40435),\n",
       " ('places', 40334),\n",
       " ('fight', 40283),\n",
       " ('bike', 40269),\n",
       " ('english', 40260),\n",
       " ('main', 40133),\n",
       " ('supposed', 40049),\n",
       " ('data', 39890),\n",
       " ('net', 39831),\n",
       " ('sad', 39741),\n",
       " ('tax', 39703),\n",
       " ('product', 39492),\n",
       " ('plan', 39434),\n",
       " ('bullshit', 39391),\n",
       " ('value', 39271),\n",
       " ('mother', 39222),\n",
       " ('health', 39215),\n",
       " ('glad', 39187),\n",
       " ('assume', 38841),\n",
       " ('major', 38821),\n",
       " ('lt', 38723),\n",
       " ('worst', 38713),\n",
       " ('related', 38706),\n",
       " ('apparently', 38529),\n",
       " ('doubt', 38471),\n",
       " ('anymore', 38426),\n",
       " ('dr', 38379),\n",
       " ('language', 38240),\n",
       " ('kinda', 38193),\n",
       " ('windows', 38142),\n",
       " ('across', 38025),\n",
       " ('position', 37994),\n",
       " ('hands', 37767),\n",
       " ('gone', 37713),\n",
       " ('bar', 37631),\n",
       " ('ways', 37558),\n",
       " ('exist', 37542),\n",
       " ('attention', 37482),\n",
       " ('doctor', 37426),\n",
       " ('effect', 37331),\n",
       " ('specific', 37301),\n",
       " ('late', 37299),\n",
       " ('eyes', 37271),\n",
       " ('known', 37267),\n",
       " ('reasons', 37231),\n",
       " ('legal', 37172),\n",
       " ('door', 37164),\n",
       " ('baby', 37162),\n",
       " ('claim', 37152),\n",
       " ('speed', 37098),\n",
       " ('lack', 37048),\n",
       " ('inside', 36893),\n",
       " ('wonder', 36836),\n",
       " ('air', 36813),\n",
       " ('gives', 36790),\n",
       " ('cat', 36721),\n",
       " ('finally', 36613),\n",
       " ('sell', 36599),\n",
       " ('dick', 36583),\n",
       " ('mention', 36534),\n",
       " ('laws', 36497),\n",
       " ('random', 36482),\n",
       " ('write', 36379),\n",
       " ('somewhere', 36290),\n",
       " ('moment', 36288),\n",
       " ('decided', 36279),\n",
       " ('eating', 36275),\n",
       " ('difficult', 36116),\n",
       " ('response', 36093),\n",
       " ('sleep', 36011),\n",
       " ('character', 35998),\n",
       " ('average', 35939),\n",
       " ('style', 35850),\n",
       " ('note', 35746),\n",
       " ('gun', 35737),\n",
       " ('available', 35721),\n",
       " ('posts', 35717),\n",
       " ('terrible', 35685),\n",
       " ('jesus', 35672),\n",
       " ('relevant', 35666),\n",
       " ('driving', 35648),\n",
       " ('pain', 35631),\n",
       " ('nobody', 35617),\n",
       " ('road', 35569),\n",
       " ('pc', 35556),\n",
       " ('telling', 35544),\n",
       " ('w', 35471),\n",
       " ('process', 35382),\n",
       " ('vote', 35252),\n",
       " ('sir', 35129),\n",
       " ('majority', 35117),\n",
       " ('choose', 35105),\n",
       " ('drunk', 35067),\n",
       " ('size', 35056),\n",
       " ('rape', 35051),\n",
       " ('l', 35013),\n",
       " ('code', 34979),\n",
       " ('fit', 34884),\n",
       " ('turned', 34838),\n",
       " ('image', 34635),\n",
       " ('energy', 34599),\n",
       " ('test', 34552),\n",
       " ('spent', 34530),\n",
       " ('paying', 34454),\n",
       " ('lives', 34375),\n",
       " ('forget', 34362),\n",
       " ('expensive', 34268),\n",
       " ('willing', 34240),\n",
       " ('uk', 34213),\n",
       " ('wear', 33988),\n",
       " ('subreddit', 33941),\n",
       " ('female', 33920),\n",
       " ('safe', 33894),\n",
       " ('q', 33882),\n",
       " ('pictures', 33851),\n",
       " ('interest', 33839),\n",
       " ('jobs', 33819),\n",
       " ('bitch', 33795),\n",
       " ('send', 33756),\n",
       " ('step', 33755),\n",
       " ('piece', 33673),\n",
       " ('truth', 33647),\n",
       " ('worry', 33637),\n",
       " ('brain', 33544),\n",
       " ('media', 33530),\n",
       " ('listen', 33489),\n",
       " ('recommend', 33403),\n",
       " ('eventually', 33399),\n",
       " ('speak', 33342),\n",
       " ('recently', 33317),\n",
       " ('extremely', 33279),\n",
       " ('decent', 33257),\n",
       " ('box', 33245),\n",
       " ('stick', 33222),\n",
       " ('strong', 33222),\n",
       " ('standard', 33124),\n",
       " ('allowed', 33104),\n",
       " ('crap', 33070),\n",
       " ('follow', 33057),\n",
       " ('season', 32910),\n",
       " ('throw', 32775),\n",
       " ('town', 32694),\n",
       " ('considered', 32691),\n",
       " ('male', 32653),\n",
       " ('apple', 32591),\n",
       " ('facebook', 32588),\n",
       " ('plenty', 32549),\n",
       " ('hurt', 32540),\n",
       " ('respect', 32469),\n",
       " ('drugs', 32465),\n",
       " ('street', 32390),\n",
       " ('putting', 32332),\n",
       " ('clean', 32330),\n",
       " ('create', 32277),\n",
       " ('moving', 32272),\n",
       " ('screen', 32227),\n",
       " ('meet', 32206),\n",
       " ('movies', 32177),\n",
       " ('beautiful', 32162),\n",
       " ('companies', 32121),\n",
       " ('towards', 32109),\n",
       " ('mentioned', 32096),\n",
       " ('lower', 32031),\n",
       " ('buying', 32003),\n",
       " ('text', 31999),\n",
       " ('girlfriend', 31965),\n",
       " ('prefer', 31956),\n",
       " ('damage', 31941),\n",
       " ('research', 31928),\n",
       " ('earth', 31864),\n",
       " ('christian', 31797),\n",
       " ('pass', 31793),\n",
       " ('drug', 31768),\n",
       " ('parts', 31757),\n",
       " ('disagree', 31736),\n",
       " ('currently', 31634),\n",
       " ('building', 31623),\n",
       " ('literally', 31550),\n",
       " ('fall', 31518),\n",
       " ('anti', 31489),\n",
       " ('complete', 31461),\n",
       " ('special', 31432),\n",
       " ('starting', 31386),\n",
       " ('date', 31382),\n",
       " ('reality', 31344),\n",
       " ('action', 31336),\n",
       " ('ability', 31304),\n",
       " ('tend', 31304),\n",
       " ('rules', 31289),\n",
       " ('beat', 31286),\n",
       " ('share', 31251),\n",
       " ('brother', 31185),\n",
       " ('none', 31168),\n",
       " ('bet', 31159),\n",
       " ('changed', 31076),\n",
       " ('blue', 31057),\n",
       " ('suggest', 30980),\n",
       " ('private', 30955),\n",
       " ('knowledge', 30900),\n",
       " ('countries', 30864),\n",
       " ('server', 30835),\n",
       " ('loved', 30820),\n",
       " ('office', 30808),\n",
       " ('troll', 30729),\n",
       " ('allow', 30701),\n",
       " ('trust', 30620),\n",
       " ('art', 30599),\n",
       " ('vs', 30573),\n",
       " ('karma', 30568),\n",
       " ('pull', 30558),\n",
       " ('summer', 30557),\n",
       " ('learned', 30543),\n",
       " ('honest', 30539),\n",
       " ('id', 30537),\n",
       " ('sign', 30526),\n",
       " ('terms', 30514),\n",
       " ('political', 30430),\n",
       " ('smoke', 30416),\n",
       " ('unfortunately', 30413),\n",
       " ('father', 30383),\n",
       " ('cars', 30369),\n",
       " ('depends', 30359),\n",
       " ('obvious', 30311),\n",
       " ('option', 30277),\n",
       " ('possibly', 30166),\n",
       " ('notice', 30136),\n",
       " ('canada', 30109),\n",
       " ('eye', 30087),\n",
       " ('older', 30042),\n",
       " ('feature', 30018),\n",
       " ('co', 30018),\n",
       " ('met', 29991),\n",
       " ('ah', 29988),\n",
       " ('result', 29949),\n",
       " ('sit', 29901),\n",
       " ('blood', 29897),\n",
       " ('morning', 29838),\n",
       " ('sucks', 29831),\n",
       " ('slow', 29808),\n",
       " ('theory', 29778),\n",
       " ('ended', 29758),\n",
       " ('avoid', 29755),\n",
       " ('involved', 29728),\n",
       " ('feels', 29711),\n",
       " ('ground', 29684),\n",
       " ('total', 29683),\n",
       " ('drop', 29679),\n",
       " ('shitty', 29642),\n",
       " ('natural', 29635),\n",
       " ('website', 29625),\n",
       " ('statement', 29606),\n",
       " ('bed', 29493),\n",
       " ('fairly', 29492),\n",
       " ('military', 29464),\n",
       " ('fix', 29460),\n",
       " ('killed', 29443),\n",
       " ('calling', 29384),\n",
       " ('quickly', 29289),\n",
       " ('paper', 29286),\n",
       " ('waiting', 29273),\n",
       " ('holy', 29243),\n",
       " ('exact', 29241),\n",
       " ('continue', 29221),\n",
       " ('helps', 29182),\n",
       " ('forever', 29173),\n",
       " ('atheist', 29145),\n",
       " ('program', 29116),\n",
       " ('fucked', 29073),\n",
       " ('sweet', 28957),\n",
       " ('info', 28831),\n",
       " ('third', 28638),\n",
       " ('oil', 28624),\n",
       " ('rate', 28582),\n",
       " ('illegal', 28574),\n",
       " ('wtf', 28572),\n",
       " ('quick', 28550),\n",
       " ('user', 28511),\n",
       " ('paul', 28510),\n",
       " ('gaming', 28507),\n",
       " ('credit', 28499),\n",
       " ('tl', 28460),\n",
       " ('million', 28413),\n",
       " ('church', 28376),\n",
       " ('comic', 28366),\n",
       " ('marriage', 28334),\n",
       " ('writing', 28307),\n",
       " ('entirely', 28304),\n",
       " ('ridiculous', 28274),\n",
       " ('written', 28254),\n",
       " ('heart', 28223),\n",
       " ('particular', 28177),\n",
       " ('including', 28171),\n",
       " ('liked', 28107),\n",
       " ('physical', 28084),\n",
       " ('race', 27992),\n",
       " ('nearly', 27986),\n",
       " ('sexual', 27951),\n",
       " ('php', 27926),\n",
       " ('provide', 27848),\n",
       " ('miss', 27836),\n",
       " ('stopped', 27827),\n",
       " ('offer', 27810),\n",
       " ('double', 27809),\n",
       " ('dark', 27671),\n",
       " ('south', 27642),\n",
       " ('risk', 27641),\n",
       " ('security', 27600),\n",
       " ('access', 27597),\n",
       " ('somehow', 27573),\n",
       " ('forward', 27518),\n",
       " ('upon', 27447),\n",
       " ('discussion', 27426),\n",
       " ('park', 27331),\n",
       " ('ride', 27300),\n",
       " ('lived', 27284),\n",
       " ('sitting', 27267),\n",
       " ('four', 27250),\n",
       " ('context', 27243),\n",
       " ('kept', 27193),\n",
       " ('posting', 27179),\n",
       " ('popular', 27178),\n",
       " ('de', 27160),\n",
       " ('voice', 27085),\n",
       " ('practice', 27084),\n",
       " ('pro', 27072),\n",
       " ('attack', 27068),\n",
       " ('culture', 27048),\n",
       " ('regardless', 27026),\n",
       " ('numbers', 27002),\n",
       " ('anywhere', 26968),\n",
       " ('indeed', 26952),\n",
       " ('ideas', 26951),\n",
       " ('field', 26946),\n",
       " ('images', 26945),\n",
       " ('regular', 26910),\n",
       " ('surprised', 26897),\n",
       " ('watched', 26884),\n",
       " ('downvote', 26882),\n",
       " ('son', 26870),\n",
       " ('bottom', 26825),\n",
       " ('fear', 26815),\n",
       " ('assuming', 26778),\n",
       " ('cover', 26771),\n",
       " ('decide', 26762),\n",
       " ('education', 26714),\n",
       " ('wall', 26705),\n",
       " ('rich', 26626),\n",
       " ('basic', 26623),\n",
       " ('taxes', 26615),\n",
       " ('conversation', 26601),\n",
       " ('base', 26570),\n",
       " ('considering', 26538),\n",
       " ('aware', 26527),\n",
       " ('universe', 26515),\n",
       " ('rule', 26512),\n",
       " ('h', 26491),\n",
       " ('knowing', 26477),\n",
       " ('stories', 26460),\n",
       " ('walking', 26457),\n",
       " ('amazon', 26397),\n",
       " ('pm', 26386),\n",
       " ('useful', 26374),\n",
       " ('accept', 26346),\n",
       " ('software', 26345),\n",
       " ('gold', 26271),\n",
       " ('gotten', 26270),\n",
       " ('married', 26205),\n",
       " ('drinking', 26130),\n",
       " ('nature', 26072),\n",
       " ('truly', 26044),\n",
       " ('bill', 26036),\n",
       " ('bible', 26012),\n",
       " ('til', 26006),\n",
       " ('porn', 25995),\n",
       " ('agreed', 25990),\n",
       " ('understanding', 25975),\n",
       " ('message', 25969),\n",
       " ('seconds', 25962),\n",
       " ('dollars', 25960),\n",
       " ('green', 25952),\n",
       " ('lead', 25898),\n",
       " ('modern', 25849),\n",
       " ('episode', 25843),\n",
       " ('multiple', 25755),\n",
       " ('film', 25753),\n",
       " ('sick', 25716),\n",
       " ('key', 25663),\n",
       " ('wearing', 25628),\n",
       " ('evil', 25611),\n",
       " ('noticed', 25560),\n",
       " ('five', 25480),\n",
       " ('grow', 25475),\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = count_words(stopword_reddit)\n",
    "word_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "258.3187446594238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_final = time.time()\n",
    "time_final-time_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "\n",
      "Collections:\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'all'\n",
      "       | \n",
      "       | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "       |   Package abc is already up-to-date!\n",
      "       | Downloading package alpino to /home/ubuntu/nltk_data...\n",
      "       |   Package alpino is already up-to-date!\n",
      "       | Downloading package biocreative_ppi to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package biocreative_ppi is already up-to-date!\n",
      "       | Downloading package brown to /home/ubuntu/nltk_data...\n",
      "       |   Package brown is already up-to-date!\n",
      "       | Downloading package brown_tei to /home/ubuntu/nltk_data...\n",
      "       |   Package brown_tei is already up-to-date!\n",
      "       | Downloading package cess_cat to /home/ubuntu/nltk_data...\n",
      "       |   Package cess_cat is already up-to-date!\n",
      "       | Downloading package cess_esp to /home/ubuntu/nltk_data...\n",
      "       |   Package cess_esp is already up-to-date!\n",
      "       | Downloading package chat80 to /home/ubuntu/nltk_data...\n",
      "       |   Package chat80 is already up-to-date!\n",
      "       | Downloading package city_database to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package city_database is already up-to-date!\n",
      "       | Downloading package cmudict to /home/ubuntu/nltk_data...\n",
      "       |   Package cmudict is already up-to-date!\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package comparative_sentences is already up-to-date!\n",
      "       | Downloading package comtrans to /home/ubuntu/nltk_data...\n",
      "       |   Package comtrans is already up-to-date!\n",
      "       | Downloading package conll2000 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2000 is already up-to-date!\n",
      "       | Downloading package conll2002 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2002 is already up-to-date!\n",
      "       | Downloading package conll2007 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2007 is already up-to-date!\n",
      "       | Downloading package crubadan to /home/ubuntu/nltk_data...\n",
      "       |   Package crubadan is already up-to-date!\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package dependency_treebank is already up-to-date!\n",
      "       | Downloading package dolch to /home/ubuntu/nltk_data...\n",
      "       |   Package dolch is already up-to-date!\n",
      "       | Downloading package europarl_raw to /home/ubuntu/nltk_data...\n",
      "       |   Package europarl_raw is already up-to-date!\n",
      "       | Downloading package floresta to /home/ubuntu/nltk_data...\n",
      "       |   Package floresta is already up-to-date!\n",
      "       | Downloading package framenet_v15 to /home/ubuntu/nltk_data...\n",
      "       |   Package framenet_v15 is already up-to-date!\n",
      "       | Downloading package framenet_v17 to /home/ubuntu/nltk_data...\n",
      "       |   Package framenet_v17 is already up-to-date!\n",
      "       | Downloading package gazetteers to /home/ubuntu/nltk_data...\n",
      "       |   Package gazetteers is already up-to-date!\n",
      "       | Downloading package genesis to /home/ubuntu/nltk_data...\n",
      "       |   Package genesis is already up-to-date!\n",
      "       | Downloading package gutenberg to /home/ubuntu/nltk_data...\n",
      "       |   Package gutenberg is already up-to-date!\n",
      "       | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "       |   Package ieer is already up-to-date!\n",
      "       | Downloading package inaugural to /home/ubuntu/nltk_data...\n",
      "       |   Package inaugural is already up-to-date!\n",
      "       | Downloading package indian to /home/ubuntu/nltk_data...\n",
      "       |   Package indian is already up-to-date!\n",
      "       | Downloading package jeita to /home/ubuntu/nltk_data...\n",
      "       |   Package jeita is already up-to-date!\n",
      "       | Downloading package kimmo to /home/ubuntu/nltk_data...\n",
      "       |   Package kimmo is already up-to-date!\n",
      "       | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "       |   Package knbc is already up-to-date!\n",
      "       | Downloading package lin_thesaurus to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package lin_thesaurus is already up-to-date!\n",
      "       | Downloading package mac_morpho to /home/ubuntu/nltk_data...\n",
      "       |   Package mac_morpho is already up-to-date!\n",
      "       | Downloading package machado to /home/ubuntu/nltk_data...\n",
      "       |   Package machado is already up-to-date!\n",
      "       | Downloading package masc_tagged to /home/ubuntu/nltk_data...\n",
      "       |   Package masc_tagged is already up-to-date!\n",
      "       | Downloading package moses_sample to /home/ubuntu/nltk_data...\n",
      "       |   Package moses_sample is already up-to-date!\n",
      "       | Downloading package movie_reviews to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package movie_reviews is already up-to-date!\n",
      "       | Downloading package names to /home/ubuntu/nltk_data...\n",
      "       |   Package names is already up-to-date!\n",
      "       | Downloading package nombank.1.0 to /home/ubuntu/nltk_data...\n",
      "       |   Package nombank.1.0 is already up-to-date!\n",
      "       | Downloading package nps_chat to /home/ubuntu/nltk_data...\n",
      "       |   Package nps_chat is already up-to-date!\n",
      "       | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "       |   Package omw is already up-to-date!\n",
      "       | Downloading package opinion_lexicon to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package opinion_lexicon is already up-to-date!\n",
      "       | Downloading package paradigms to /home/ubuntu/nltk_data...\n",
      "       |   Package paradigms is already up-to-date!\n",
      "       | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "       |   Package pil is already up-to-date!\n",
      "       | Downloading package pl196x to /home/ubuntu/nltk_data...\n",
      "       |   Package pl196x is already up-to-date!\n",
      "       | Downloading package ppattach to /home/ubuntu/nltk_data...\n",
      "       |   Package ppattach is already up-to-date!\n",
      "       | Downloading package problem_reports to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package problem_reports is already up-to-date!\n",
      "       | Downloading package propbank to /home/ubuntu/nltk_data...\n",
      "       |   Package propbank is already up-to-date!\n",
      "       | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "       |   Package ptb is already up-to-date!\n",
      "       | Downloading package product_reviews_1 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package product_reviews_1 is already up-to-date!\n",
      "       | Downloading package product_reviews_2 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package product_reviews_2 is already up-to-date!\n",
      "       | Downloading package pros_cons to /home/ubuntu/nltk_data...\n",
      "       |   Package pros_cons is already up-to-date!\n",
      "       | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "       |   Package qc is already up-to-date!\n",
      "       | Downloading package reuters to /home/ubuntu/nltk_data...\n",
      "       |   Package reuters is already up-to-date!\n",
      "       | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "       |   Package rte is already up-to-date!\n",
      "       | Downloading package semcor to /home/ubuntu/nltk_data...\n",
      "       |   Package semcor is already up-to-date!\n",
      "       | Downloading package senseval to /home/ubuntu/nltk_data...\n",
      "       |   Package senseval is already up-to-date!\n",
      "       | Downloading package sentiwordnet to /home/ubuntu/nltk_data...\n",
      "       |   Package sentiwordnet is already up-to-date!\n",
      "       | Downloading package sentence_polarity to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sentence_polarity is already up-to-date!\n",
      "       | Downloading package shakespeare to /home/ubuntu/nltk_data...\n",
      "       |   Package shakespeare is already up-to-date!\n",
      "       | Downloading package sinica_treebank to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sinica_treebank is already up-to-date!\n",
      "       | Downloading package smultron to /home/ubuntu/nltk_data...\n",
      "       |   Package smultron is already up-to-date!\n",
      "       | Downloading package state_union to /home/ubuntu/nltk_data...\n",
      "       |   Package state_union is already up-to-date!\n",
      "       | Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "       |   Package stopwords is already up-to-date!\n",
      "       | Downloading package subjectivity to /home/ubuntu/nltk_data...\n",
      "       |   Package subjectivity is already up-to-date!\n",
      "       | Downloading package swadesh to /home/ubuntu/nltk_data...\n",
      "       |   Package swadesh is already up-to-date!\n",
      "       | Downloading package switchboard to /home/ubuntu/nltk_data...\n",
      "       |   Package switchboard is already up-to-date!\n",
      "       | Downloading package timit to /home/ubuntu/nltk_data...\n",
      "       |   Package timit is already up-to-date!\n",
      "       | Downloading package toolbox to /home/ubuntu/nltk_data...\n",
      "       |   Package toolbox is already up-to-date!\n",
      "       | Downloading package treebank to /home/ubuntu/nltk_data...\n",
      "       |   Package treebank is already up-to-date!\n",
      "       | Downloading package twitter_samples to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package twitter_samples is already up-to-date!\n",
      "       | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "       |   Package udhr is already up-to-date!\n",
      "       | Downloading package udhr2 to /home/ubuntu/nltk_data...\n",
      "       |   Package udhr2 is already up-to-date!\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package unicode_samples is already up-to-date!\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package universal_treebanks_v20 is already up-to-date!\n",
      "       | Downloading package verbnet to /home/ubuntu/nltk_data...\n",
      "       |   Package verbnet is already up-to-date!\n",
      "       | Downloading package verbnet3 to /home/ubuntu/nltk_data...\n",
      "       |   Package verbnet3 is already up-to-date!\n",
      "       | Downloading package webtext to /home/ubuntu/nltk_data...\n",
      "       |   Package webtext is already up-to-date!\n",
      "       | Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "       |   Package wordnet is already up-to-date!\n",
      "       | Downloading package wordnet_ic to /home/ubuntu/nltk_data...\n",
      "       |   Package wordnet_ic is already up-to-date!\n",
      "       | Downloading package words to /home/ubuntu/nltk_data...\n",
      "       |   Package words is already up-to-date!\n",
      "       | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "       |   Package ycoe is already up-to-date!\n",
      "       | Downloading package rslp to /home/ubuntu/nltk_data...\n",
      "       |   Package rslp is already up-to-date!\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package universal_tagset is already up-to-date!\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package maxent_ne_chunker is already up-to-date!\n",
      "       | Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package book_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package book_grammars is already up-to-date!\n",
      "       | Downloading package sample_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sample_grammars is already up-to-date!\n",
      "       | Downloading package spanish_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package spanish_grammars is already up-to-date!\n",
      "       | Downloading package basque_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package basque_grammars is already up-to-date!\n",
      "       | Downloading package large_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package large_grammars is already up-to-date!\n",
      "       | Downloading package tagsets to /home/ubuntu/nltk_data...\n",
      "       |   Package tagsets is already up-to-date!\n",
      "       | Downloading package snowball_data to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package snowball_data is already up-to-date!\n",
      "       | Downloading package bllip_wsj_no_aux to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "       | Downloading package word2vec_sample to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package word2vec_sample is already up-to-date!\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package panlex_swadesh is already up-to-date!\n",
      "       | Downloading package mte_teip5 to /home/ubuntu/nltk_data...\n",
      "       |   Package mte_teip5 is already up-to-date!\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "       |       date!\n",
      "       | Downloading package perluniprops to /home/ubuntu/nltk_data...\n",
      "       |   Package perluniprops is already up-to-date!\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package nonbreaking_prefixes is already up-to-date!\n",
      "       | Downloading package vader_lexicon to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package vader_lexicon is already up-to-date!\n",
      "       | Downloading package porter_test to /home/ubuntu/nltk_data...\n",
      "       |   Package porter_test is already up-to-date!\n",
      "       | Downloading package wmt15_eval to /home/ubuntu/nltk_data...\n",
      "       |   Package wmt15_eval is already up-to-date!\n",
      "       | Downloading package mwa_ppdb to /home/ubuntu/nltk_data...\n",
      "       |   Package mwa_ppdb is already up-to-date!\n",
      "       | \n",
      "     Done downloading collection all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> l\n",
      "Packages:\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "\n",
      "Collections:\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> nltk.corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Error loading nltk.corpus: Package 'nltk.corpus' not found in\n",
      "        index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> nltk all\n",
      "Command 'nltk all' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [*] abc................. Australian Broadcasting Commission 2006\n",
      "  [*] alpino.............. Alpino Dutch Treebank\n",
      "  [*] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [*] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [*] basque_grammars..... Grammars for Basque\n",
      "  [*] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [*] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [*] book_grammars....... Grammars from NLTK Book\n",
      "  [*] brown............... Brown Corpus\n",
      "  [*] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [*] cess_cat............ CESS-CAT Treebank\n",
      "  [*] cess_esp............ CESS-ESP Treebank\n",
      "  [*] chat80.............. Chat-80 Data Files\n",
      "  [*] city_database....... City Database\n",
      "  [*] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [*] comparative_sentences Comparative Sentence Dataset\n",
      "  [*] comtrans............ ComTrans Corpus Sample\n",
      "  [*] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [*] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [*] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [*] crubadan............ Crubadan Corpus\n",
      "  [*] dependency_treebank. Dependency Parsed Treebank\n",
      "  [*] dolch............... Dolch Word List\n",
      "  [*] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [*] floresta............ Portuguese Treebank\n",
      "  [*] framenet_v15........ FrameNet 1.5\n",
      "  [*] framenet_v17........ FrameNet 1.7\n",
      "  [*] gazetteers.......... Gazeteer Lists\n",
      "  [*] genesis............. Genesis Corpus\n",
      "  [*] gutenberg........... Project Gutenberg Selections\n",
      "  [*] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [*] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [*] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [*] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [*] kimmo............... PC-KIMMO Data Files\n",
      "  [*] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [*] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [*] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [*] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [*] machado............. Machado de Assis -- Obra Completa\n",
      "  [*] masc_tagged......... MASC Tagged Corpus\n",
      "  [*] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [*] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [*] moses_sample........ Moses Sample Models\n",
      "  [*] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [*] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [*] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [*] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [*] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [*] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [*] nps_chat............ NPS Chat\n",
      "  [*] omw................. Open Multilingual Wordnet\n",
      "  [*] opinion_lexicon..... Opinion Lexicon\n",
      "  [*] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [*] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [*] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [*] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [*] pl196x.............. Polish language of the XX century sixties\n",
      "  [*] porter_test......... Porter Stemmer Test Files\n",
      "  [*] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [*] problem_reports..... Problem Report Corpus\n",
      "  [*] product_reviews_1... Product Reviews (5 Products)\n",
      "  [*] product_reviews_2... Product Reviews (9 Products)\n",
      "  [*] propbank............ Proposition Bank Corpus 1.0\n",
      "  [*] pros_cons........... Pros and Cons\n",
      "  [*] ptb................. Penn Treebank\n",
      "  [*] punkt............... Punkt Tokenizer Models\n",
      "  [*] qc.................. Experimental Data for Question Classification\n",
      "  [*] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [*] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [*] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [*] sample_grammars..... Sample Grammars\n",
      "  [*] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [*] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [*] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [*] sentiwordnet........ SentiWordNet\n",
      "  [*] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [*] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [*] smultron............ SMULTRON Corpus Sample\n",
      "  [*] snowball_data....... Snowball Data\n",
      "  [*] spanish_grammars.... Grammars for Spanish\n",
      "  [*] state_union......... C-Span State of the Union Address Corpus\n",
      "  [*] stopwords........... Stopwords Corpus\n",
      "  [*] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [*] swadesh............. Swadesh Wordlists\n",
      "  [*] switchboard......... Switchboard Corpus Sample\n",
      "  [*] tagsets............. Help on Tagsets\n",
      "  [*] timit............... TIMIT Corpus Sample\n",
      "  [*] toolbox............. Toolbox Sample Files\n",
      "  [*] treebank............ Penn Treebank Sample\n",
      "  [*] twitter_samples..... Twitter Samples\n",
      "  [*] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [*] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [*] unicode_samples..... Unicode Samples\n",
      "  [*] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [*] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [*] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [*] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [*] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [*] webtext............. Web Text Corpus\n",
      "  [*] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [*] word2vec_sample..... Word2Vec Sample\n",
      "  [*] wordnet............. WordNet\n",
      "  [*] wordnet_ic.......... WordNet-InfoContent\n",
      "  [*] words............... Word Lists\n",
      "  [*] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [*] all-corpora......... All the corpora\n",
      "  [*] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [*] all................. All packages\n",
      "  [*] book................ Everything used in the NLTK Book\n",
      "  [*] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [*] tests............... Packages for running tests\n",
      "  [*] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> \n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> all-nltk\n",
      "Command 'all-nltk' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> all-nltk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading collection 'all-nltk'\n",
      "       | \n",
      "       | Downloading package abc to /home/ubuntu/nltk_data...\n",
      "       |   Package abc is already up-to-date!\n",
      "       | Downloading package alpino to /home/ubuntu/nltk_data...\n",
      "       |   Package alpino is already up-to-date!\n",
      "       | Downloading package biocreative_ppi to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package biocreative_ppi is already up-to-date!\n",
      "       | Downloading package brown to /home/ubuntu/nltk_data...\n",
      "       |   Package brown is already up-to-date!\n",
      "       | Downloading package brown_tei to /home/ubuntu/nltk_data...\n",
      "       |   Package brown_tei is already up-to-date!\n",
      "       | Downloading package cess_cat to /home/ubuntu/nltk_data...\n",
      "       |   Package cess_cat is already up-to-date!\n",
      "       | Downloading package cess_esp to /home/ubuntu/nltk_data...\n",
      "       |   Package cess_esp is already up-to-date!\n",
      "       | Downloading package chat80 to /home/ubuntu/nltk_data...\n",
      "       |   Package chat80 is already up-to-date!\n",
      "       | Downloading package city_database to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package city_database is already up-to-date!\n",
      "       | Downloading package cmudict to /home/ubuntu/nltk_data...\n",
      "       |   Package cmudict is already up-to-date!\n",
      "       | Downloading package comparative_sentences to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package comparative_sentences is already up-to-date!\n",
      "       | Downloading package comtrans to /home/ubuntu/nltk_data...\n",
      "       |   Package comtrans is already up-to-date!\n",
      "       | Downloading package conll2000 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2000 is already up-to-date!\n",
      "       | Downloading package conll2002 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2002 is already up-to-date!\n",
      "       | Downloading package conll2007 to /home/ubuntu/nltk_data...\n",
      "       |   Package conll2007 is already up-to-date!\n",
      "       | Downloading package crubadan to /home/ubuntu/nltk_data...\n",
      "       |   Package crubadan is already up-to-date!\n",
      "       | Downloading package dependency_treebank to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package dependency_treebank is already up-to-date!\n",
      "       | Downloading package europarl_raw to /home/ubuntu/nltk_data...\n",
      "       |   Package europarl_raw is already up-to-date!\n",
      "       | Downloading package floresta to /home/ubuntu/nltk_data...\n",
      "       |   Package floresta is already up-to-date!\n",
      "       | Downloading package framenet_v15 to /home/ubuntu/nltk_data...\n",
      "       |   Package framenet_v15 is already up-to-date!\n",
      "       | Downloading package framenet_v17 to /home/ubuntu/nltk_data...\n",
      "       |   Package framenet_v17 is already up-to-date!\n",
      "       | Downloading package gazetteers to /home/ubuntu/nltk_data...\n",
      "       |   Package gazetteers is already up-to-date!\n",
      "       | Downloading package genesis to /home/ubuntu/nltk_data...\n",
      "       |   Package genesis is already up-to-date!\n",
      "       | Downloading package gutenberg to /home/ubuntu/nltk_data...\n",
      "       |   Package gutenberg is already up-to-date!\n",
      "       | Downloading package ieer to /home/ubuntu/nltk_data...\n",
      "       |   Package ieer is already up-to-date!\n",
      "       | Downloading package inaugural to /home/ubuntu/nltk_data...\n",
      "       |   Package inaugural is already up-to-date!\n",
      "       | Downloading package indian to /home/ubuntu/nltk_data...\n",
      "       |   Package indian is already up-to-date!\n",
      "       | Downloading package jeita to /home/ubuntu/nltk_data...\n",
      "       |   Package jeita is already up-to-date!\n",
      "       | Downloading package kimmo to /home/ubuntu/nltk_data...\n",
      "       |   Package kimmo is already up-to-date!\n",
      "       | Downloading package knbc to /home/ubuntu/nltk_data...\n",
      "       |   Package knbc is already up-to-date!\n",
      "       | Downloading package lin_thesaurus to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package lin_thesaurus is already up-to-date!\n",
      "       | Downloading package mac_morpho to /home/ubuntu/nltk_data...\n",
      "       |   Package mac_morpho is already up-to-date!\n",
      "       | Downloading package machado to /home/ubuntu/nltk_data...\n",
      "       |   Package machado is already up-to-date!\n",
      "       | Downloading package masc_tagged to /home/ubuntu/nltk_data...\n",
      "       |   Package masc_tagged is already up-to-date!\n",
      "       | Downloading package moses_sample to /home/ubuntu/nltk_data...\n",
      "       |   Package moses_sample is already up-to-date!\n",
      "       | Downloading package movie_reviews to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package movie_reviews is already up-to-date!\n",
      "       | Downloading package names to /home/ubuntu/nltk_data...\n",
      "       |   Package names is already up-to-date!\n",
      "       | Downloading package nombank.1.0 to /home/ubuntu/nltk_data...\n",
      "       |   Package nombank.1.0 is already up-to-date!\n",
      "       | Downloading package nps_chat to /home/ubuntu/nltk_data...\n",
      "       |   Package nps_chat is already up-to-date!\n",
      "       | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "       |   Package omw is already up-to-date!\n",
      "       | Downloading package opinion_lexicon to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package opinion_lexicon is already up-to-date!\n",
      "       | Downloading package paradigms to /home/ubuntu/nltk_data...\n",
      "       |   Package paradigms is already up-to-date!\n",
      "       | Downloading package pil to /home/ubuntu/nltk_data...\n",
      "       |   Package pil is already up-to-date!\n",
      "       | Downloading package pl196x to /home/ubuntu/nltk_data...\n",
      "       |   Package pl196x is already up-to-date!\n",
      "       | Downloading package ppattach to /home/ubuntu/nltk_data...\n",
      "       |   Package ppattach is already up-to-date!\n",
      "       | Downloading package problem_reports to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package problem_reports is already up-to-date!\n",
      "       | Downloading package propbank to /home/ubuntu/nltk_data...\n",
      "       |   Package propbank is already up-to-date!\n",
      "       | Downloading package ptb to /home/ubuntu/nltk_data...\n",
      "       |   Package ptb is already up-to-date!\n",
      "       | Downloading package product_reviews_1 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package product_reviews_1 is already up-to-date!\n",
      "       | Downloading package product_reviews_2 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package product_reviews_2 is already up-to-date!\n",
      "       | Downloading package pros_cons to /home/ubuntu/nltk_data...\n",
      "       |   Package pros_cons is already up-to-date!\n",
      "       | Downloading package qc to /home/ubuntu/nltk_data...\n",
      "       |   Package qc is already up-to-date!\n",
      "       | Downloading package reuters to /home/ubuntu/nltk_data...\n",
      "       |   Package reuters is already up-to-date!\n",
      "       | Downloading package rte to /home/ubuntu/nltk_data...\n",
      "       |   Package rte is already up-to-date!\n",
      "       | Downloading package semcor to /home/ubuntu/nltk_data...\n",
      "       |   Package semcor is already up-to-date!\n",
      "       | Downloading package senseval to /home/ubuntu/nltk_data...\n",
      "       |   Package senseval is already up-to-date!\n",
      "       | Downloading package sentiwordnet to /home/ubuntu/nltk_data...\n",
      "       |   Package sentiwordnet is already up-to-date!\n",
      "       | Downloading package sentence_polarity to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sentence_polarity is already up-to-date!\n",
      "       | Downloading package shakespeare to /home/ubuntu/nltk_data...\n",
      "       |   Package shakespeare is already up-to-date!\n",
      "       | Downloading package sinica_treebank to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sinica_treebank is already up-to-date!\n",
      "       | Downloading package smultron to /home/ubuntu/nltk_data...\n",
      "       |   Package smultron is already up-to-date!\n",
      "       | Downloading package state_union to /home/ubuntu/nltk_data...\n",
      "       |   Package state_union is already up-to-date!\n",
      "       | Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "       |   Package stopwords is already up-to-date!\n",
      "       | Downloading package subjectivity to /home/ubuntu/nltk_data...\n",
      "       |   Package subjectivity is already up-to-date!\n",
      "       | Downloading package swadesh to /home/ubuntu/nltk_data...\n",
      "       |   Package swadesh is already up-to-date!\n",
      "       | Downloading package switchboard to /home/ubuntu/nltk_data...\n",
      "       |   Package switchboard is already up-to-date!\n",
      "       | Downloading package timit to /home/ubuntu/nltk_data...\n",
      "       |   Package timit is already up-to-date!\n",
      "       | Downloading package toolbox to /home/ubuntu/nltk_data...\n",
      "       |   Package toolbox is already up-to-date!\n",
      "       | Downloading package treebank to /home/ubuntu/nltk_data...\n",
      "       |   Package treebank is already up-to-date!\n",
      "       | Downloading package twitter_samples to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package twitter_samples is already up-to-date!\n",
      "       | Downloading package udhr to /home/ubuntu/nltk_data...\n",
      "       |   Package udhr is already up-to-date!\n",
      "       | Downloading package udhr2 to /home/ubuntu/nltk_data...\n",
      "       |   Package udhr2 is already up-to-date!\n",
      "       | Downloading package unicode_samples to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package unicode_samples is already up-to-date!\n",
      "       | Downloading package universal_treebanks_v20 to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package universal_treebanks_v20 is already up-to-date!\n",
      "       | Downloading package verbnet to /home/ubuntu/nltk_data...\n",
      "       |   Package verbnet is already up-to-date!\n",
      "       | Downloading package verbnet3 to /home/ubuntu/nltk_data...\n",
      "       |   Package verbnet3 is already up-to-date!\n",
      "       | Downloading package webtext to /home/ubuntu/nltk_data...\n",
      "       |   Package webtext is already up-to-date!\n",
      "       | Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "       |   Package wordnet is already up-to-date!\n",
      "       | Downloading package wordnet_ic to /home/ubuntu/nltk_data...\n",
      "       |   Package wordnet_ic is already up-to-date!\n",
      "       | Downloading package words to /home/ubuntu/nltk_data...\n",
      "       |   Package words is already up-to-date!\n",
      "       | Downloading package ycoe to /home/ubuntu/nltk_data...\n",
      "       |   Package ycoe is already up-to-date!\n",
      "       | Downloading package rslp to /home/ubuntu/nltk_data...\n",
      "       |   Package rslp is already up-to-date!\n",
      "       | Downloading package maxent_treebank_pos_tagger to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package maxent_treebank_pos_tagger is already up-to-date!\n",
      "       | Downloading package universal_tagset to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package universal_tagset is already up-to-date!\n",
      "       | Downloading package maxent_ne_chunker to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package maxent_ne_chunker is already up-to-date!\n",
      "       | Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "       |   Package punkt is already up-to-date!\n",
      "       | Downloading package book_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package book_grammars is already up-to-date!\n",
      "       | Downloading package sample_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package sample_grammars is already up-to-date!\n",
      "       | Downloading package spanish_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package spanish_grammars is already up-to-date!\n",
      "       | Downloading package basque_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package basque_grammars is already up-to-date!\n",
      "       | Downloading package large_grammars to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package large_grammars is already up-to-date!\n",
      "       | Downloading package tagsets to /home/ubuntu/nltk_data...\n",
      "       |   Package tagsets is already up-to-date!\n",
      "       | Downloading package snowball_data to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package snowball_data is already up-to-date!\n",
      "       | Downloading package bllip_wsj_no_aux to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "       | Downloading package word2vec_sample to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package word2vec_sample is already up-to-date!\n",
      "       | Downloading package panlex_swadesh to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package panlex_swadesh is already up-to-date!\n",
      "       | Downloading package mte_teip5 to /home/ubuntu/nltk_data...\n",
      "       |   Package mte_teip5 is already up-to-date!\n",
      "       | Downloading package averaged_perceptron_tagger to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package averaged_perceptron_tagger is already up-to-date!\n",
      "       | Downloading package averaged_perceptron_tagger_ru to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "       |       date!\n",
      "       | Downloading package perluniprops to /home/ubuntu/nltk_data...\n",
      "       |   Package perluniprops is already up-to-date!\n",
      "       | Downloading package nonbreaking_prefixes to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package nonbreaking_prefixes is already up-to-date!\n",
      "       | Downloading package vader_lexicon to\n",
      "       |     /home/ubuntu/nltk_data...\n",
      "       |   Package vader_lexicon is already up-to-date!\n",
      "       | Downloading package porter_test to /home/ubuntu/nltk_data...\n",
      "       |   Package porter_test is already up-to-date!\n",
      "       | Downloading package wmt15_eval to /home/ubuntu/nltk_data...\n",
      "       |   Package wmt15_eval is already up-to-date!\n",
      "       | Downloading package mwa_ppdb to /home/ubuntu/nltk_data...\n",
      "       |   Package mwa_ppdb is already up-to-date!\n",
      "       | \n",
      "     Done downloading collection all-nltk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's not garbage, its fear mongering.\",\n",
       " \"Actually, it's nice to have a new idea too, especially when you say stuff about how your thing is gonna do for the web what the Mac did for the personal computer. If there was any evidence of that kind of thing to be smelled anywhere, I think you'd find a little more willingness to wait and see on the execution.\",\n",
       " \"I've read them, and they are not that funny. I would suggest you go to bash.org and read top 100, those are much better.\",\n",
       " \"If you look at a lot of these changes, they are really extracted refinements rather than bloating additions. They aim at making things more DRY and eliminating workarounds that used to be more clumsy. So while the Rails API may gain a few methods, it will actually make the programmer's experience using Rails more consistent and streamlined.\\r\\n\\r\\nThe beauty of a framework like rails is that you can use it without leveraging its full api - go ahead and write your stuff 'old skool' in 15 lines of code, and a few days later, you'll look over that code and you'll slash it down to a mere 3-4 lines using a rails method you've just discovered - it's a lot of fun.\\r\\n\\r\\nDHH said somewhere that Rails 1.0 should have been 2.0, because he doesn't see a lot of major additions any time soon. He's wisely pushed 'higher level' functionality (like login systems, etc.) into generators, plugins, engines, whatnot, to keep the framework uncluttered. Rails is walking the line between high level convenience and low level flexibility better than any other framework I've ever used.\",\n",
       " \"That's true, but who else would?\",\n",
       " 'High school drop out does good',\n",
       " '[deleted]',\n",
       " 'They put the power switch and all of the connections in the back?  Or is that the front?',\n",
       " '手元のiCalでも化けていたので、記事の通りに対処した。',\n",
       " 'or, download the [bugmenot extension](http://reddit.com/info?id=23yu)...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_rdd_02 = df_02.select(\"body\").rdd.flatMap(lambda x: x)\n",
    "reddit_rdd_02.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 's', 'not', 'garbage', 'its', 'fear', 'mongering'],\n",
       " ['actually',\n",
       "  'it',\n",
       "  's',\n",
       "  'nice',\n",
       "  'to',\n",
       "  'have',\n",
       "  'a',\n",
       "  'new',\n",
       "  'idea',\n",
       "  'too',\n",
       "  'especially',\n",
       "  'when',\n",
       "  'you',\n",
       "  'say',\n",
       "  'stuff',\n",
       "  'about',\n",
       "  'how',\n",
       "  'your',\n",
       "  'thing',\n",
       "  'is',\n",
       "  'gonna',\n",
       "  'do',\n",
       "  'for',\n",
       "  'the',\n",
       "  'web',\n",
       "  'what',\n",
       "  'the',\n",
       "  'mac',\n",
       "  'did',\n",
       "  'for',\n",
       "  'the',\n",
       "  'personal',\n",
       "  'computer',\n",
       "  'if',\n",
       "  'there',\n",
       "  'was',\n",
       "  'any',\n",
       "  'evidence',\n",
       "  'of',\n",
       "  'that',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'thing',\n",
       "  'to',\n",
       "  'be',\n",
       "  'smelled',\n",
       "  'anywhere',\n",
       "  'i',\n",
       "  'think',\n",
       "  'you',\n",
       "  'd',\n",
       "  'find',\n",
       "  'a',\n",
       "  'little',\n",
       "  'more',\n",
       "  'willingness',\n",
       "  'to',\n",
       "  'wait',\n",
       "  'and',\n",
       "  'see',\n",
       "  'on',\n",
       "  'the',\n",
       "  'execution'],\n",
       " ['i',\n",
       "  've',\n",
       "  'read',\n",
       "  'them',\n",
       "  'and',\n",
       "  'they',\n",
       "  'are',\n",
       "  'not',\n",
       "  'that',\n",
       "  'funny',\n",
       "  'i',\n",
       "  'would',\n",
       "  'suggest',\n",
       "  'you',\n",
       "  'go',\n",
       "  'to',\n",
       "  'bash',\n",
       "  'org',\n",
       "  'and',\n",
       "  'read',\n",
       "  'top',\n",
       "  'those',\n",
       "  'are',\n",
       "  'much',\n",
       "  'better'],\n",
       " ['if',\n",
       "  'you',\n",
       "  'look',\n",
       "  'at',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'these',\n",
       "  'changes',\n",
       "  'they',\n",
       "  'are',\n",
       "  'really',\n",
       "  'extracted',\n",
       "  'refinements',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'bloating',\n",
       "  'additions',\n",
       "  'they',\n",
       "  'aim',\n",
       "  'at',\n",
       "  'making',\n",
       "  'things',\n",
       "  'more',\n",
       "  'dry',\n",
       "  'and',\n",
       "  'eliminating',\n",
       "  'workarounds',\n",
       "  'that',\n",
       "  'used',\n",
       "  'to',\n",
       "  'be',\n",
       "  'more',\n",
       "  'clumsy',\n",
       "  'so',\n",
       "  'while',\n",
       "  'the',\n",
       "  'rails',\n",
       "  'api',\n",
       "  'may',\n",
       "  'gain',\n",
       "  'a',\n",
       "  'few',\n",
       "  'methods',\n",
       "  'it',\n",
       "  'will',\n",
       "  'actually',\n",
       "  'make',\n",
       "  'the',\n",
       "  'programmer',\n",
       "  's',\n",
       "  'experience',\n",
       "  'using',\n",
       "  'rails',\n",
       "  'more',\n",
       "  'consistent',\n",
       "  'and',\n",
       "  'streamlined',\n",
       "  'the',\n",
       "  'beauty',\n",
       "  'of',\n",
       "  'a',\n",
       "  'framework',\n",
       "  'like',\n",
       "  'rails',\n",
       "  'is',\n",
       "  'that',\n",
       "  'you',\n",
       "  'can',\n",
       "  'use',\n",
       "  'it',\n",
       "  'without',\n",
       "  'leveraging',\n",
       "  'its',\n",
       "  'full',\n",
       "  'api',\n",
       "  'go',\n",
       "  'ahead',\n",
       "  'and',\n",
       "  'write',\n",
       "  'your',\n",
       "  'stuff',\n",
       "  'old',\n",
       "  'skool',\n",
       "  'in',\n",
       "  'lines',\n",
       "  'of',\n",
       "  'code',\n",
       "  'and',\n",
       "  'a',\n",
       "  'few',\n",
       "  'days',\n",
       "  'later',\n",
       "  'you',\n",
       "  'll',\n",
       "  'look',\n",
       "  'over',\n",
       "  'that',\n",
       "  'code',\n",
       "  'and',\n",
       "  'you',\n",
       "  'll',\n",
       "  'slash',\n",
       "  'it',\n",
       "  'down',\n",
       "  'to',\n",
       "  'a',\n",
       "  'mere',\n",
       "  'lines',\n",
       "  'using',\n",
       "  'a',\n",
       "  'rails',\n",
       "  'method',\n",
       "  'you',\n",
       "  've',\n",
       "  'just',\n",
       "  'discovered',\n",
       "  'it',\n",
       "  's',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'fun',\n",
       "  'dhh',\n",
       "  'said',\n",
       "  'somewhere',\n",
       "  'that',\n",
       "  'rails',\n",
       "  'should',\n",
       "  'have',\n",
       "  'been',\n",
       "  'because',\n",
       "  'he',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'see',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'major',\n",
       "  'additions',\n",
       "  'any',\n",
       "  'time',\n",
       "  'soon',\n",
       "  'he',\n",
       "  's',\n",
       "  'wisely',\n",
       "  'pushed',\n",
       "  'higher',\n",
       "  'level',\n",
       "  'functionality',\n",
       "  'like',\n",
       "  'login',\n",
       "  'systems',\n",
       "  'etc',\n",
       "  'into',\n",
       "  'generators',\n",
       "  'plugins',\n",
       "  'engines',\n",
       "  'whatnot',\n",
       "  'to',\n",
       "  'keep',\n",
       "  'the',\n",
       "  'framework',\n",
       "  'uncluttered',\n",
       "  'rails',\n",
       "  'is',\n",
       "  'walking',\n",
       "  'the',\n",
       "  'line',\n",
       "  'between',\n",
       "  'high',\n",
       "  'level',\n",
       "  'convenience',\n",
       "  'and',\n",
       "  'low',\n",
       "  'level',\n",
       "  'flexibility',\n",
       "  'better',\n",
       "  'than',\n",
       "  'any',\n",
       "  'other',\n",
       "  'framework',\n",
       "  'i',\n",
       "  've',\n",
       "  'ever',\n",
       "  'used'],\n",
       " ['that', 's', 'true', 'but', 'who', 'else', 'would'],\n",
       " ['high', 'school', 'drop', 'out', 'does', 'good'],\n",
       " ['they',\n",
       "  'put',\n",
       "  'the',\n",
       "  'power',\n",
       "  'switch',\n",
       "  'and',\n",
       "  'all',\n",
       "  'of',\n",
       "  'the',\n",
       "  'connections',\n",
       "  'in',\n",
       "  'the',\n",
       "  'back',\n",
       "  'or',\n",
       "  'is',\n",
       "  'that',\n",
       "  'the',\n",
       "  'front'],\n",
       " ['ical'],\n",
       " ['or',\n",
       "  'download',\n",
       "  'the',\n",
       "  'bugmenot',\n",
       "  'extension',\n",
       "  'http',\n",
       "  'reddit',\n",
       "  'com',\n",
       "  'info',\n",
       "  'id',\n",
       "  'yu'],\n",
       " ['there',\n",
       "  's',\n",
       "  'some',\n",
       "  'confusion',\n",
       "  'as',\n",
       "  'of',\n",
       "  'late',\n",
       "  'http',\n",
       "  'features',\n",
       "  'reddit',\n",
       "  'com',\n",
       "  'info',\n",
       "  'id',\n",
       "  'b',\n",
       "  'as',\n",
       "  'to',\n",
       "  'what',\n",
       "  'the',\n",
       "  'voting',\n",
       "  'up',\n",
       "  'down',\n",
       "  'means',\n",
       "  'voting',\n",
       "  'up',\n",
       "  'may',\n",
       "  'mean',\n",
       "  'for',\n",
       "  'example',\n",
       "  'that',\n",
       "  'you',\n",
       "  'are',\n",
       "  'glad',\n",
       "  'reddit',\n",
       "  'directed',\n",
       "  'you',\n",
       "  'to',\n",
       "  'the',\n",
       "  'article',\n",
       "  'site',\n",
       "  'etc',\n",
       "  'it',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'necessarily',\n",
       "  'mean',\n",
       "  'that',\n",
       "  'you',\n",
       "  'agreed',\n",
       "  'with',\n",
       "  'the',\n",
       "  'post',\n",
       "  'for',\n",
       "  'example']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_words_02 = reddit_rdd_022.map(lower_rdd)\n",
    "reddit_words_02.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It's not garbage, its fear mongering.\",\n",
       " \"Actually, it's nice to have a new idea too, especially when you say stuff about how your thing is gonna do for the web what the Mac did for the personal computer. If there was any evidence of that kind of thing to be smelled anywhere, I think you'd find a little more willingness to wait and see on the execution.\",\n",
       " \"I've read them, and they are not that funny. I would suggest you go to bash.org and read top 100, those are much better.\",\n",
       " \"If you look at a lot of these changes, they are really extracted refinements rather than bloating additions. They aim at making things more DRY and eliminating workarounds that used to be more clumsy. So while the Rails API may gain a few methods, it will actually make the programmer's experience using Rails more consistent and streamlined.\\r\\n\\r\\nThe beauty of a framework like rails is that you can use it without leveraging its full api - go ahead and write your stuff 'old skool' in 15 lines of code, and a few days later, you'll look over that code and you'll slash it down to a mere 3-4 lines using a rails method you've just discovered - it's a lot of fun.\\r\\n\\r\\nDHH said somewhere that Rails 1.0 should have been 2.0, because he doesn't see a lot of major additions any time soon. He's wisely pushed 'higher level' functionality (like login systems, etc.) into generators, plugins, engines, whatnot, to keep the framework uncluttered. Rails is walking the line between high level convenience and low level flexibility better than any other framework I've ever used.\",\n",
       " \"That's true, but who else would?\",\n",
       " 'High school drop out does good',\n",
       " 'They put the power switch and all of the connections in the back?  Or is that the front?',\n",
       " '手元のiCalでも化けていたので、記事の通りに対処した。',\n",
       " 'or, download the [bugmenot extension](http://reddit.com/info?id=23yu)...',\n",
       " \"there's some confusion as of late (http://features.reddit.com/info?id=27b9) as to what the voting up/down means.  Voting up may mean, for example, that you are glad reddit directed you to the article/site/etc.  It doesn't necessarily mean that you agreed with the post, for example.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit_rdd_022 = reddit_rdd_02.filter(lambda row: row != \"[deleted]\")\n",
    "reddit_rdd_022.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['garbage', 'fear', 'mongering'],\n",
       " ['actually',\n",
       "  'nice',\n",
       "  'new',\n",
       "  'idea',\n",
       "  'especially',\n",
       "  'say',\n",
       "  'stuff',\n",
       "  'thing',\n",
       "  'gonna',\n",
       "  'web',\n",
       "  'mac',\n",
       "  'personal',\n",
       "  'computer',\n",
       "  'evidence',\n",
       "  'kind',\n",
       "  'thing',\n",
       "  'smelled',\n",
       "  'anywhere',\n",
       "  'think',\n",
       "  'find',\n",
       "  'little',\n",
       "  'willingness',\n",
       "  'wait',\n",
       "  'see',\n",
       "  'execution'],\n",
       " ['read',\n",
       "  'funny',\n",
       "  'would',\n",
       "  'suggest',\n",
       "  'go',\n",
       "  'bash',\n",
       "  'org',\n",
       "  'read',\n",
       "  'top',\n",
       "  'much',\n",
       "  'better'],\n",
       " ['look',\n",
       "  'lot',\n",
       "  'changes',\n",
       "  'really',\n",
       "  'extracted',\n",
       "  'refinements',\n",
       "  'rather',\n",
       "  'bloating',\n",
       "  'additions',\n",
       "  'aim',\n",
       "  'making',\n",
       "  'things',\n",
       "  'dry',\n",
       "  'eliminating',\n",
       "  'workarounds',\n",
       "  'used',\n",
       "  'clumsy',\n",
       "  'rails',\n",
       "  'api',\n",
       "  'may',\n",
       "  'gain',\n",
       "  'methods',\n",
       "  'actually',\n",
       "  'make',\n",
       "  'programmer',\n",
       "  'experience',\n",
       "  'using',\n",
       "  'rails',\n",
       "  'consistent',\n",
       "  'streamlined',\n",
       "  'beauty',\n",
       "  'framework',\n",
       "  'like',\n",
       "  'rails',\n",
       "  'use',\n",
       "  'without',\n",
       "  'leveraging',\n",
       "  'full',\n",
       "  'api',\n",
       "  'go',\n",
       "  'ahead',\n",
       "  'write',\n",
       "  'stuff',\n",
       "  'old',\n",
       "  'skool',\n",
       "  'lines',\n",
       "  'code',\n",
       "  'days',\n",
       "  'later',\n",
       "  'look',\n",
       "  'code',\n",
       "  'slash',\n",
       "  'mere',\n",
       "  'lines',\n",
       "  'using',\n",
       "  'rails',\n",
       "  'method',\n",
       "  'discovered',\n",
       "  'lot',\n",
       "  'fun',\n",
       "  'dhh',\n",
       "  'said',\n",
       "  'somewhere',\n",
       "  'rails',\n",
       "  'see',\n",
       "  'lot',\n",
       "  'major',\n",
       "  'additions',\n",
       "  'time',\n",
       "  'soon',\n",
       "  'wisely',\n",
       "  'pushed',\n",
       "  'higher',\n",
       "  'level',\n",
       "  'functionality',\n",
       "  'like',\n",
       "  'login',\n",
       "  'systems',\n",
       "  'etc',\n",
       "  'generators',\n",
       "  'plugins',\n",
       "  'engines',\n",
       "  'whatnot',\n",
       "  'keep',\n",
       "  'framework',\n",
       "  'uncluttered',\n",
       "  'rails',\n",
       "  'walking',\n",
       "  'line',\n",
       "  'high',\n",
       "  'level',\n",
       "  'convenience',\n",
       "  'low',\n",
       "  'level',\n",
       "  'flexibility',\n",
       "  'better',\n",
       "  'framework',\n",
       "  'ever',\n",
       "  'used'],\n",
       " ['true', 'else', 'would'],\n",
       " ['high', 'school', 'drop', 'good'],\n",
       " ['put', 'power', 'switch', 'connections', 'back', 'front'],\n",
       " ['ical'],\n",
       " ['download', 'bugmenot', 'extension', 'reddit', 'info', 'id', 'yu'],\n",
       " ['confusion',\n",
       "  'late',\n",
       "  'features',\n",
       "  'reddit',\n",
       "  'info',\n",
       "  'id',\n",
       "  'b',\n",
       "  'voting',\n",
       "  'means',\n",
       "  'voting',\n",
       "  'may',\n",
       "  'mean',\n",
       "  'example',\n",
       "  'glad',\n",
       "  'reddit',\n",
       "  'directed',\n",
       "  'article',\n",
       "  'site',\n",
       "  'etc',\n",
       "  'necessarily',\n",
       "  'mean',\n",
       "  'agreed',\n",
       "  'post',\n",
       "  'example']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_reddit_02 = reddit_words_02.map(removeStopWordsFunct)\n",
    "stopword_reddit_02.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 ms, sys: 4.4 ms, total: 33.7 ms\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word_counts_02 = count_words(stopword_reddit_02)\n",
    "word_counts_02.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives a single float value\n",
    "psutil.cpu_percent()\n",
    "# gives an object with many fields\n",
    "psutil.virtual_memory()\n",
    "# you can convert that object to a dictionary \n",
    "dict(psutil.virtual_memory()._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
